{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11462898,"sourceType":"datasetVersion","datasetId":7127458}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# V1","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# PROJECT: ADVANCED INDOOR MICROBIAL RISK ASSESSMENT SYSTEM (8-WEEK PROJECT SCOPE)\n# VERSION: 4.1 (Hyperopt Error Fix)\n#\n# KEY UPGRADES:\n# 1. SCIENTIFIC RE-FRAMING: The model no longer predicts a non-existent \"mould\"\n#    outcome. It now predicts a scientifically valid, multi-class \"Environmental State\"\n#    based on ASHRAE/building science principles, serving as an early warning system.\n# 2. HIERARCHICAL STATE DEFINITION: The target variable is now defined by a\n#    prioritized set of rules (Sustained Dampness > Poor Ventilation > Elevated Humidity > Optimal).\n# 3. ROBUST PIPELINE: The existing advanced features and ML pipeline are now\n#    leveraged to predict these meaningful, real-world environmental states.\n# ==============================================================================\n\n# ------------------------------------------------------------------------------\n# SECTION A: SETUP AND ENVIRONMENT\n# ------------------------------------------------------------------------------\n# âœ… Install compatible versions for stability\n# !pip install --quiet numpy pandas scikit-learn imbalanced-learn xgboost shap hyperopt seaborn\n\n# âœ… Core imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport shap\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# âœ… Scikit-learn, Imblearn, XGBoost, Hyperopt\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom sklearn.ensemble import IsolationForest\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\n# âœ… Reproducibility\nSEED = 42\nnp.random.seed(SEED)\n\nclass MicrobialRiskAssessor:\n    \"\"\"\n    An advanced pipeline for assessing indoor environmental states conducive to\n    microbial risk, acting as an early warning system.\n    \"\"\"\n    def __init__(self, file_path, roll_window=24, rh_threshold=80.0):\n        self.file_path = file_path\n        self.roll_window = roll_window\n        self.rh_threshold = rh_threshold # Humidity % for \"wet\" conditions\n        self.df = None\n        self.X = None\n        self.y = None\n        self.model = None\n        self.scaler = None\n        self.encoder = None\n        self.feature_names = None\n        print(\"âœ… MicrobialRiskAssessor initialized.\")\n\n    def _load_and_clean_data(self):\n        \"\"\"Loads and cleans the initial dataset.\"\"\"\n        print(\"--> STAGE 1: Loading and Cleaning Data...\")\n        self.df = pd.read_csv(self.file_path)\n        \n        self.df.rename(columns=lambda c: \n                       c.replace('(?C)', '').replace('(%)', '').replace('(ppm)', '')\n                       .replace('(?g/m?)', '').replace('(ppb)', '').replace('(lux)', '')\n                       .strip().replace(' ', '_'), \n                       inplace=True)\n        \n        self.df['Timestamp'] = pd.to_datetime(self.df['Timestamp'], dayfirst=True)\n        self.df.set_index('Timestamp', inplace=True)\n        self.df.sort_index(inplace=True)\n        self.df.ffill(inplace=True)\n        print(\"    Data loaded and cleaned.\")\n\n    def _engineer_features_advanced(self):\n        \"\"\"Generates advanced features including volatility, lag, and anomaly scores.\"\"\"\n        print(\"--> STAGE 2: Advanced Feature Engineering...\")\n        # Volatility Features\n        self.df['Hum_24_std'] = self.df['Humidity'].rolling(self.roll_window, min_periods=1).std()\n        self.df['CO2_24_std'] = self.df['CO2'].rolling(self.roll_window, min_periods=1).std()\n\n        # Time of Wetness (ToW) Calculation: Cumulative hours above RH threshold in the last 3 days\n        is_wet = (self.df['Humidity'] > self.rh_threshold).astype(int)\n        self.df['Time_of_Wetness_72h'] = is_wet.rolling(window=self.roll_window * 3, min_periods=1).sum()\n        \n        # Unsupervised Anomaly Detection (Isolation Forest)\n        print(\"    Running Isolation Forest for anomaly detection...\")\n        numeric_cols = self.df.select_dtypes(include=np.number).columns\n        if_model = IsolationForest(contamination=0.01, random_state=SEED)\n        if_model.fit(self.df[numeric_cols])\n        self.df['anomaly_score'] = if_model.decision_function(self.df[numeric_cols])\n        \n        self.df.ffill(inplace=True)\n        print(\"    Advanced features created.\")\n\n    def _define_environmental_state(self):\n        \"\"\"\n        Defines the target variable based on a hierarchy of scientifically-backed\n        environmental states conducive to microbial growth.\n        \"\"\"\n        print(\"--> STAGE 3: Defining Environmental State (Target Variable)...\")\n\n        def get_state(row):\n            # Priority 1: Sustained Dampness (most critical)\n            if row['Time_of_Wetness_72h'] > 4: # More than 4 hours of high RH in last 3 days\n                return 'Sustained_Dampness'\n            # Priority 2: Poor Ventilation (major contributing factor)\n            elif row['CO2'] > 1000: # ASHRAE guideline for acceptable CO2\n                return 'Poor_Ventilation'\n            # Priority 3: Elevated Humidity (initial warning sign)\n            elif row['Humidity'] > 60: # ASHRAE guideline for humidity\n                return 'Elevated_Humidity'\n            # Default State: Optimal Conditions\n            else:\n                return 'Optimal'\n\n        self.df['Environmental_State'] = self.df.apply(get_state, axis=1)\n        print(\"    Environmental states defined.\")\n        print(\"\\nEnvironmental State Distribution:\")\n        print(self.df['Environmental_State'].value_counts())\n\n    def _prepare_for_modeling(self):\n        \"\"\"Prepares final X, y dataframes, scales, and balances the data.\"\"\"\n        print(\"--> STAGE 4: Preparing Data for Final Model...\")\n        self.df.dropna(inplace=True)\n        self.y = self.df['Environmental_State']\n        self.X = self.df.drop(columns=['Environmental_State']) # Drop the target\n        self.X = pd.get_dummies(self.X, columns=['Ventilation_Status'], drop_first=True)\n        self.feature_names = self.X.columns.tolist()\n\n        self.encoder = LabelEncoder()\n        y_encoded = self.encoder.fit_transform(self.y)\n\n        if len(self.encoder.classes_) < 2:\n            raise ValueError(\"The target variable has less than 2 classes. Cannot proceed.\")\n\n        split_idx = int(len(self.X) * 0.8)\n        X_train, self.X_test = self.X.iloc[:split_idx], self.X.iloc[split_idx:]\n        y_train, self.y_test = y_encoded[:split_idx], y_encoded[split_idx:]\n\n        self.scaler = StandardScaler()\n        X_train_scaled = self.scaler.fit_transform(X_train)\n        self.X_test_scaled = self.scaler.transform(self.X_test)\n\n        sm = SMOTE(random_state=SEED)\n        self.X_train_bal, self.y_train_bal = sm.fit_resample(X_train_scaled, y_train)\n        print(f\"    Data prepared. Training set balanced with SMOTE: {self.X_train_bal.shape}\")\n\n    def _tune_and_train_model(self):\n        \"\"\"Uses Hyperopt to find the best XGBoost parameters and trains the final model.\"\"\"\n        print(\"--> STAGE 5: Hyperparameter Tuning and Training...\")\n        \n        # --- FIX: Create a dedicated validation set for Hyperopt to avoid the __sklearn_tags__ error ---\n        # This split is only for the optimization process.\n        X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n            self.X_train_bal, self.y_train_bal, test_size=0.2, random_state=SEED\n        )\n        \n        space = {\n            'n_estimators': hp.quniform('n_estimators', 100, 800, 50),\n            'max_depth': hp.quniform('max_depth', 4, 15, 1),\n            'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n            'gamma': hp.uniform('gamma', 0, 0.5)\n        }\n\n        def objective(params):\n            params['n_estimators'] = int(params['n_estimators'])\n            params['max_depth'] = int(params['max_depth'])\n            \n            clf = XGBClassifier(**params, objective='multi:softmax', num_class=len(self.encoder.classes_),\n                                use_label_encoder=False, eval_metric='mlogloss', random_state=SEED, n_jobs=-1)\n            \n            # Train on the smaller optimization training set\n            clf.fit(X_train_opt, y_train_opt)\n            \n            # Evaluate on the dedicated validation set\n            y_pred = clf.predict(X_val_opt)\n            score = f1_score(y_val_opt, y_pred, average='weighted')\n            \n            # Hyperopt minimizes the loss, so we return the negative of the F1 score\n            return {'loss': -score, 'status': STATUS_OK}\n\n        trials = Trials()\n        print(\"    Running Bayesian Optimization (Hyperopt)...\")\n        best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials, show_progressbar=False)\n        best['n_estimators'] = int(best['n_estimators'])\n        best['max_depth'] = int(best['max_depth'])\n        print(f\"    Best parameters found: {best}\")\n\n        # Train the final model on the ENTIRE balanced training set with the best parameters\n        self.model = XGBClassifier(**best, objective='multi:softmax', num_class=len(self.encoder.classes_),\n                                   use_label_encoder=False, eval_metric='mlogloss', random_state=SEED, n_jobs=-1)\n        self.model.fit(self.X_train_bal, self.y_train_bal)\n        print(\"    Final model trained successfully.\")\n\n    def _evaluate_and_explain(self):\n        \"\"\"Evaluates the model and generates SHAP explanations.\"\"\"\n        print(\"--> STAGE 6: Model Evaluation and SHAP Explainability...\")\n        y_pred = self.model.predict(self.X_test_scaled)\n        y_pred_labels = self.encoder.inverse_transform(y_pred)\n        y_true_labels = self.encoder.inverse_transform(self.y_test)\n\n        print(\"\\nðŸ“Š Final Model Classification Report:\\n\")\n        print(classification_report(y_true_labels, y_pred_labels, zero_division=0))\n        \n        cm = confusion_matrix(y_true_labels, y_pred_labels, labels=self.encoder.classes_)\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=self.encoder.classes_, yticklabels=self.encoder.classes_)\n        plt.title(\"Confusion Matrix\")\n        plt.xlabel(\"Predicted\")\n        plt.ylabel(\"Actual\")\n        plt.show()\n\n        print(\"    Generating SHAP plots...\")\n        explainer = shap.TreeExplainer(self.model)\n        # Use the full balanced training set for SHAP values for a complete picture\n        shap_values = explainer.shap_values(self.X_train_bal)\n        \n        X_train_bal_df = pd.DataFrame(self.X_train_bal, columns=self.feature_names)\n        \n        plt.figure()\n        shap.summary_plot(shap_values, X_train_bal_df, plot_type=\"bar\", show=True)\n        plt.figure()\n        shap.summary_plot(shap_values, X_train_bal_df, show=True)\n\n    def save_artifacts(self, model_path='environmental_state_model.joblib', scaler_path='state_scaler.joblib', encoder_path='state_encoder.joblib'):\n        \"\"\"Saves the trained model, scaler, and encoder.\"\"\"\n        print(\"--> STAGE 7: Saving Model Artifacts...\")\n        joblib.dump(self.model, model_path)\n        joblib.dump(self.scaler, scaler_path)\n        joblib.dump(self.encoder, encoder_path)\n        print(f\"    Model saved to {model_path}\")\n        print(f\"    Scaler saved to {scaler_path}\")\n        print(f\"    Encoder saved to {encoder_path}\")\n\n    def run_pipeline(self):\n        \"\"\"Executes the full pipeline from data loading to saving artifacts.\"\"\"\n        self._load_and_clean_data()\n        self._engineer_features_advanced()\n        self._define_environmental_state()\n        self._prepare_for_modeling()\n        self._tune_and_train_model()\n        self._evaluate_and_explain()\n        self.save_artifacts()\n        print(\"\\nâœ…âœ…âœ… Full pipeline executed successfully! âœ…âœ…âœ…\")\n\n# ------------------------------------------------------------------------------\n# SECTION B: EXECUTION\n# ------------------------------------------------------------------------------\nif __name__ == '__main__':\n    try:\n        assessor = MicrobialRiskAssessor(file_path='/kaggle/input/iot-indoor-air-quality/IoT_Indoor_Air_Quality_Dataset.csv')\n        assessor.run_pipeline()\n    except FileNotFoundError:\n        print(\"\\n\\n---\")\n        print(\"FATAL ERROR: 'IoT_Indoor_Air_Quality_Dataset.csv' not found.\")\n        print(\"Please make sure the dataset file is uploaded and available in the same directory.\")\n        print(\"---\")\n    except Exception as e:\n        print(f\"\\n\\n--- AN UNEXPECTED ERROR OCCURRED ---\")\n        print(f\"Error Type: {type(e).__name__}\")\n        print(f\"Error Details: {e}\")\n        print(\"--------------------------------------\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# V2","metadata":{}},{"cell_type":"code","source":"# ------------------------------------------------------------------------------\n# SECTION A: SETUP AND ENVIRONMENT\n# ------------------------------------------------------------------------------\n# âœ… Install compatible versions for stability\n# !pip install --quiet numpy pandas scikit-learn imbalanced-learn xgboost shap hyperopt seaborn\n\n# âœ… Core imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport shap\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# âœ… Scikit-learn, Imblearn, XGBoost, Hyperopt\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom sklearn.ensemble import IsolationForest\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\n# âœ… Reproducibility\nSEED = 42\nnp.random.seed(SEED)\n\nclass MicrobialRiskAssessor:\n    \"\"\"\n    An advanced pipeline for assessing indoor environmental states conducive to\n    microbial risk, acting as an early warning system.\n    \"\"\"\n    def __init__(self, file_path, roll_window=24, rh_threshold=80.0):\n        self.file_path = file_path\n        self.roll_window = roll_window\n        self.rh_threshold = rh_threshold # Humidity % for \"wet\" conditions\n        self.df = None\n        self.X = None\n        self.y = None\n        self.model = None\n        self.scaler = None\n        self.encoder = None\n        self.feature_names = None\n        self.explainer = None\n        print(\"âœ… MicrobialRiskAssessor initialized.\")\n\n    def _load_and_clean_data(self):\n        \"\"\"Loads and cleans the initial dataset.\"\"\"\n        print(\"--> STAGE 1: Loading and Cleaning Data...\")\n        self.df = pd.read_csv(self.file_path)\n        \n        self.df.rename(columns=lambda c: \n                       c.replace('(?C)', '').replace('(%)', '').replace('(ppm)', '')\n                       .replace('(?g/m?)', '').replace('(ppb)', '').replace('(lux)', '')\n                       .strip().replace(' ', '_'), \n                       inplace=True)\n        \n        self.df['Timestamp'] = pd.to_datetime(self.df['Timestamp'], dayfirst=True)\n        self.df.set_index('Timestamp', inplace=True)\n        self.df.sort_index(inplace=True)\n        self.df.ffill(inplace=True)\n        print(\"    Data loaded and cleaned.\")\n\n    def _engineer_features_advanced(self):\n        \"\"\"Generates advanced features including volatility, lag, and anomaly scores.\"\"\"\n        print(\"--> STAGE 2: Advanced Feature Engineering...\")\n        self.df['Hum_24_std'] = self.df['Humidity'].rolling(self.roll_window, min_periods=1).std()\n        self.df['CO2_24_std'] = self.df['CO2'].rolling(self.roll_window, min_periods=1).std()\n\n        is_wet = (self.df['Humidity'] > self.rh_threshold).astype(int)\n        self.df['Time_of_Wetness_72h'] = is_wet.rolling(window=self.roll_window * 3, min_periods=1).sum()\n        \n        print(\"    Running Isolation Forest for anomaly detection...\")\n        numeric_cols = self.df.select_dtypes(include=np.number).columns\n        if_model = IsolationForest(contamination=0.01, random_state=SEED)\n        if_model.fit(self.df[numeric_cols])\n        self.df['anomaly_score'] = if_model.decision_function(self.df[numeric_cols])\n        \n        self.df.ffill(inplace=True)\n        print(\"    Advanced features created.\")\n\n    def _define_environmental_state(self):\n        \"\"\"Defines the target variable based on a hierarchy of scientifically-backed states.\"\"\"\n        print(\"--> STAGE 3: Defining Environmental State (Target Variable)...\")\n\n        def get_state(row):\n            if row['Time_of_Wetness_72h'] > 4: return 'Sustained_Dampness'\n            elif row['CO2'] > 1000: return 'Poor_Ventilation'\n            elif row['Humidity'] > 60: return 'Elevated_Humidity'\n            else: return 'Optimal'\n\n        self.df['Environmental_State'] = self.df.apply(get_state, axis=1)\n        print(\"    Environmental states defined.\")\n        print(\"\\nEnvironmental State Distribution:\")\n        print(self.df['Environmental_State'].value_counts())\n\n    def _prepare_for_modeling(self):\n        \"\"\"Prepares final X, y dataframes, scales, and balances the data.\"\"\"\n        print(\"--> STAGE 4: Preparing Data for Final Model...\")\n        self.df.dropna(inplace=True)\n        self.y = self.df['Environmental_State']\n        self.X = self.df.drop(columns=['Environmental_State'])\n        self.X = pd.get_dummies(self.X, columns=['Ventilation_Status'], drop_first=True)\n        self.feature_names = self.X.columns.tolist()\n\n        self.encoder = LabelEncoder()\n        y_encoded = self.encoder.fit_transform(self.y)\n\n        if len(self.encoder.classes_) < 2:\n            raise ValueError(\"The target variable has less than 2 classes. Cannot proceed.\")\n\n        split_idx = int(len(self.X) * 0.8)\n        X_train, self.X_test = self.X.iloc[:split_idx], self.X.iloc[split_idx:]\n        y_train, self.y_test = y_encoded[:split_idx], y_encoded[split_idx:]\n\n        self.scaler = StandardScaler()\n        X_train_scaled = self.scaler.fit_transform(X_train)\n        self.X_test_scaled = self.scaler.transform(self.X_test)\n\n        sm = SMOTE(random_state=SEED)\n        self.X_train_bal, self.y_train_bal = sm.fit_resample(X_train_scaled, y_train)\n        print(f\"    Data prepared. Training set balanced with SMOTE: {self.X_train_bal.shape}\")\n\n    def _tune_and_train_model(self):\n        \"\"\"Uses Hyperopt to find the best XGBoost parameters and trains the final model.\"\"\"\n        print(\"--> STAGE 5: Hyperparameter Tuning and Training...\")\n        \n        X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n            self.X_train_bal, self.y_train_bal, test_size=0.2, random_state=SEED)\n        \n        space = {'n_estimators': hp.quniform('n_estimators', 100, 800, 50),\n                 'max_depth': hp.quniform('max_depth', 4, 15, 1),\n                 'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n                 'gamma': hp.uniform('gamma', 0, 0.5)}\n\n        def objective(params):\n            params['n_estimators'] = int(params['n_estimators'])\n            params['max_depth'] = int(params['max_depth'])\n            clf = XGBClassifier(**params, objective='multi:softmax', num_class=len(self.encoder.classes_),\n                                use_label_encoder=False, eval_metric='mlogloss', random_state=SEED, n_jobs=-1)\n            clf.fit(X_train_opt, y_train_opt)\n            y_pred = clf.predict(X_val_opt)\n            score = f1_score(y_val_opt, y_pred, average='weighted')\n            return {'loss': -score, 'status': STATUS_OK}\n\n        trials = Trials()\n        print(\"    Running Bayesian Optimization (Hyperopt)...\")\n        best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials, show_progressbar=False)\n        best['n_estimators'] = int(best['n_estimators']); best['max_depth'] = int(best['max_depth'])\n        print(f\"    Best parameters found: {best}\")\n\n        self.model = XGBClassifier(**best, objective='multi:softmax', num_class=len(self.encoder.classes_),\n                                   use_label_encoder=False, eval_metric='mlogloss', random_state=SEED, n_jobs=-1)\n        self.model.fit(self.X_train_bal, self.y_train_bal)\n        print(\"    Final model trained successfully.\")\n\n    def _evaluate_and_explain(self):\n        \"\"\"Evaluates the model and generates SHAP explanations.\"\"\"\n        print(\"--> STAGE 6: Model Evaluation and SHAP Explainability...\")\n        y_pred = self.model.predict(self.X_test_scaled)\n        y_pred_labels = self.encoder.inverse_transform(y_pred)\n        y_true_labels = self.encoder.inverse_transform(self.y_test)\n\n        print(\"\\nðŸ“Š Final Model Classification Report:\\n\")\n        print(classification_report(y_true_labels, y_pred_labels, zero_division=0))\n        \n        cm = confusion_matrix(y_true_labels, y_pred_labels, labels=self.encoder.classes_)\n        plt.figure(figsize=(8, 6)); sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=self.encoder.classes_, yticklabels=self.encoder.classes_)\n        plt.title(\"Confusion Matrix\"); plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\"); plt.show()\n\n        print(\"    Generating SHAP plots...\")\n        self.explainer = shap.TreeExplainer(self.model)\n        shap_values = self.explainer.shap_values(self.X_train_bal)\n        X_train_bal_df = pd.DataFrame(self.X_train_bal, columns=self.feature_names)\n        plt.figure(); shap.summary_plot(shap_values, X_train_bal_df, plot_type=\"bar\", show=True)\n        plt.figure(); shap.summary_plot(shap_values, X_train_bal_df, show=True)\n\n    def save_artifacts(self, model_path='environmental_state_model.joblib', scaler_path='state_scaler.joblib', encoder_path='state_encoder.joblib'):\n        \"\"\"Saves the trained model, scaler, and encoder.\"\"\"\n        print(\"--> STAGE 7: Saving Model Artifacts...\")\n        joblib.dump(self.model, model_path)\n        joblib.dump(self.scaler, scaler_path)\n        joblib.dump(self.encoder, encoder_path)\n        joblib.dump(self.feature_names, 'feature_names.joblib')\n        print(f\"    Model, Scaler, Encoder, and Feature List saved.\")\n\n    def run_pipeline(self):\n        \"\"\"Executes the full pipeline from data loading to saving artifacts.\"\"\"\n        self._load_and_clean_data()\n        self._engineer_features_advanced()\n        self._define_environmental_state()\n        self._prepare_for_modeling()\n        self._tune_and_train_model()\n        self._evaluate_and_explain()\n        self.save_artifacts()\n        print(\"\\nâœ…âœ…âœ… Full pipeline executed successfully! âœ…âœ…âœ…\")\n\n    def predict_and_explain_single_instance(self, input_data,\n                                             model_path='environmental_state_model.joblib', \n                                             scaler_path='state_scaler.joblib', \n                                             encoder_path='state_encoder.joblib',\n                                             features_path='feature_names.joblib'):\n        \"\"\"\n        Loads the trained model and artifacts to predict the environmental state\n        for a single new data point and explain the prediction.\n        \"\"\"\n        print(\"\\n--> NEW PREDICTION: Loading artifacts for single instance test...\")\n        try:\n            model = joblib.load(model_path)\n            scaler = joblib.load(scaler_path)\n            encoder = joblib.load(encoder_path)\n            feature_names = joblib.load(features_path)\n        except FileNotFoundError as e:\n            print(f\"    ERROR: Could not load a required artifact: {e}\")\n            print(\"    Please run the full training pipeline first.\")\n            return\n\n        print(\"    Artifacts loaded successfully.\")\n        \n        # Create a DataFrame from the input dictionary\n        input_df = pd.DataFrame([input_data])\n        \n        # One-hot encode ventilation status\n        input_df['Ventilation_Status_Open'] = 1 if input_data.get('Ventilation_Status') == 'Open' else 0\n        if 'Ventilation_Status' in input_df.columns:\n            input_df = input_df.drop(columns=['Ventilation_Status'])\n            \n        # Ensure all required feature columns exist, filling missing ones with 0\n        for col in feature_names:\n            if col not in input_df.columns:\n                input_df[col] = 0\n        \n        # Reorder columns to match the training data\n        input_df = input_df[feature_names]\n        \n        # Scale the data using the loaded scaler\n        input_scaled = scaler.transform(input_df)\n        \n        # Make the prediction\n        prediction_encoded = model.predict(input_scaled)[0]\n        prediction_proba = model.predict_proba(input_scaled)[0]\n        prediction_label = encoder.inverse_transform([prediction_encoded])[0]\n        \n        print(\"\\n--- PREDICTION RESULT ---\")\n        print(f\"Predicted Environmental State: {prediction_label.upper()}\")\n        print(\"\\nConfidence Scores:\")\n        for i, class_name in enumerate(encoder.classes_):\n            print(f\"  - {class_name}: {prediction_proba[i]*100:.2f}%\")\n            \n        # --- SHAP Explanation ---\n        print(\"\\n--- PREDICTION EXPLANATION ---\")\n        explainer = shap.TreeExplainer(model)\n        shap_values = explainer.shap_values(input_scaled)\n        \n        # Find the index for the predicted class\n        class_index = list(encoder.classes_).index(prediction_label)\n        \n        print(f\"The plot below shows the factors that pushed the model's prediction towards '{prediction_label}'.\")\n        print(\"Features in RED push the prediction HIGHER (towards this outcome).\")\n        print(\"Features in BLUE push the prediction LOWER (away from this outcome).\")\n        \n        shap.initjs()\n        display(shap.force_plot(explainer.expected_value[class_index], \n                                shap_values[class_index], \n                                pd.DataFrame(input_scaled, columns=feature_names), \n                                matplotlib=True))\n\n# ------------------------------------------------------------------------------\n# SECTION B: EXECUTION & TESTING\n# ------------------------------------------------------------------------------\nif __name__ == '__main__':\n    try:\n        assessor = MicrobialRiskAssessor(file_path='/kaggle/input/iot-indoor-air-quality/IoT_Indoor_Air_Quality_Dataset.csv')\n        # Step 1: Run the full training pipeline first to create the model artifacts\n        assessor.run_pipeline()\n\n        # Step 2: Now, test the trained model with new, hypothetical scenarios\n        print(\"\\n\\n===================================================================\")\n        print(\"      RUNNING PREDICTION & EXPLANATION ON TEST SCENARIOS\")\n        print(\"===================================================================\")\n\n        # SCENARIO 1: Optimal Conditions\n        optimal_conditions = {\n            'Temperature': 21.0, 'Humidity': 50.0, 'CO2': 600, 'PM2.5': 10, 'PM10': 15,\n            'TVOC': 150, 'CO': 5, 'Light_Intensity': 500, 'Motion_Detected': 1,\n            'Occupancy_Count': 2, 'Ventilation_Status': 'Open',\n            'Hum_24_std': 2.5, 'CO2_24_std': 50, 'Time_of_Wetness_72h': 0, 'anomaly_score': 0.1\n        }\n        assessor.predict_and_explain_single_instance(optimal_conditions)\n\n        # SCENARIO 2: Poor Ventilation Scenario\n        poor_ventilation = {\n            'Temperature': 23.0, 'Humidity': 58.0, 'CO2': 1500, 'PM2.5': 25, 'PM10': 30,\n            'TVOC': 400, 'CO': 8, 'Light_Intensity': 200, 'Motion_Detected': 1,\n            'Occupancy_Count': 4, 'Ventilation_Status': 'Closed',\n            'Hum_24_std': 3.0, 'CO2_24_std': 200, 'Time_of_Wetness_72h': 0, 'anomaly_score': 0.0\n        }\n        assessor.predict_and_explain_single_instance(poor_ventilation)\n        \n        # SCENARIO 3: High Humidity Scenario\n        high_humidity = {\n            'Temperature': 22.0, 'Humidity': 75.0, 'CO2': 800, 'PM2.5': 15, 'PM10': 20,\n            'TVOC': 250, 'CO': 6, 'Light_Intensity': 100, 'Motion_Detected': 0,\n            'Occupancy_Count': 1, 'Ventilation_Status': 'Closed',\n            'Hum_24_std': 5.0, 'CO2_24_std': 70, 'Time_of_Wetness_72h': 2, 'anomaly_score': -0.1\n        }\n        assessor.predict_and_explain_single_instance(high_humidity)\n\n    except FileNotFoundError:\n        print(\"\\n\\n---\")\n        print(\"FATAL ERROR: 'IoT_Indoor_Air_Quality_Dataset.csv' not found.\")\n        print(\"Please make sure the dataset file is uploaded and available in the same directory.\")\n        print(\"---\")\n    except Exception as e:\n        print(f\"\\n\\n--- AN UNEXPECTED ERROR OCCURRED ---\")\n        print(f\"Error Type: {type(e).__name__}\")\n        print(f\"Error Details: {e}\")\n        print(\"--------------------------------------\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# V3","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# PROJECT: ADVANCED INDOOR MICROBIAL RISK ASSESSMENT SYSTEM (8-WEEK PROJECT SCOPE)\n# VERSION: 8.1 (GPU Acceleration Enabled)\n#\n# KEY UPGRADES:\n# 1. TRULY HOLISTIC DIAGNOSTICS: This definitive version assesses ALL relevant\n#    parameters (Temp, Humidity, CO2, PM2.5, PM10, TVOC, CO, Light) against\n#    multi-level scientific ranges simultaneously.\n# 2. ALL POSSIBLE COMBINATIONS: The target state is now a dynamic combination of\n#    every identified issue, creating a powerful and precise diagnostic label.\n# 3. EXPLICIT SCIENTIFIC GROUNDING: Every threshold is commented with its source\n#    (ASHRAE, WHO, EPA), ensuring the model's logic is transparent.\n# 4. GPU ACCELERATION: Added the 'tree_method':'gpu_hist' parameter to leverage\n#    NVIDIA GPUs for significantly faster model training.\n# ==============================================================================\n\n# ------------------------------------------------------------------------------\n# SECTION A: SETUP AND ENVIRONMENT\n# ------------------------------------------------------------------------------\n# âœ… Install compatible versions for stability\n!pip install --quiet numpy pandas scikit-learn imbalanced-learn xgboost shap hyperopt seaborn\n\n# âœ… Core imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport shap\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n# âœ… Scikit-learn, Imblearn, XGBoost, Hyperopt\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report, confusion_matrix, f1_score\nfrom sklearn.ensemble import IsolationForest\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\nfrom hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n\n# âœ… Reproducibility\nSEED = 42\nnp.random.seed(SEED)\n\nclass EnvironmentalDiagnosticEngine:\n    \"\"\"\n    An advanced pipeline that trains a model to diagnose a wide range of\n    concurrent indoor environmental issues using multi-level, range-based\n    scientific standards.\n    \"\"\"\n    def __init__(self, file_path, roll_window=24, rh_threshold=80.0):\n        self.file_path = file_path\n        self.roll_window = roll_window\n        self.rh_threshold = rh_threshold\n        self.df = None\n        self.X = None\n        self.y = None\n        self.model = None\n        self.scaler = None\n        self.encoder = None\n        self.feature_names = None\n        self.explainer = None\n        print(\"âœ… EnvironmentalDiagnosticEngine initialized.\")\n\n    def _load_and_clean_data(self):\n        \"\"\"Loads and cleans the initial dataset.\"\"\"\n        print(\"--> STAGE 1: Loading and Cleaning Data...\")\n        self.df = pd.read_csv(self.file_path)\n        \n        self.df.rename(columns=lambda c: \n                       c.replace('(?C)', '').replace('(%)', '').replace('(ppm)', '')\n                       .replace('(?g/m?)', '').replace('(ppb)', '').replace('(lux)', '')\n                       .strip().replace(' ', '_'), \n                       inplace=True)\n        \n        self.df['Timestamp'] = pd.to_datetime(self.df['Timestamp'], dayfirst=True)\n        self.df.set_index('Timestamp', inplace=True)\n        self.df.sort_index(inplace=True)\n        self.df.ffill(inplace=True)\n        print(\"    Data loaded and cleaned.\")\n\n    def _engineer_features_advanced(self):\n        \"\"\"Generates advanced features that will be used as clues by the model.\"\"\"\n        print(\"--> STAGE 2: Advanced Feature Engineering...\")\n        self.df['Hum_24_std'] = self.df['Humidity'].rolling(self.roll_window, min_periods=1).std()\n        self.df['CO2_24_std'] = self.df['CO2'].rolling(self.roll_window, min_periods=1).std()\n\n        is_wet = (self.df['Humidity'] > self.rh_threshold).astype(int)\n        self.df['Time_of_Wetness_72h'] = is_wet.rolling(window=self.roll_window * 3, min_periods=1).sum()\n        \n        print(\"    Running Isolation Forest for anomaly detection...\")\n        numeric_cols = self.df.select_dtypes(include=np.number).columns\n        if_model = IsolationForest(contamination=0.01, random_state=SEED)\n        if_model.fit(self.df[numeric_cols])\n        self.df['anomaly_score'] = if_model.decision_function(self.df[numeric_cols])\n        \n        self.df.ffill(inplace=True)\n        print(\"    Advanced features created.\")\n\n    def _define_holistic_environmental_state(self):\n        \"\"\"\n        Defines a holistic target variable by checking all relevant parameters against\n        multi-level, range-based scientific standards. This is the core of the expert system.\n        \"\"\"\n        print(\"--> STAGE 3: Defining Holistic Environmental State (Target Variable)...\")\n\n        def get_holistic_state(row):\n            issues = []\n            \n            # --- Temperature (Based on ASHRAE 55 Thermal Comfort Standard) ---\n            temp = row['Temperature']\n            if temp > 29: issues.append('Temp-Hot')\n            elif temp > 26: issues.append('Temp-Warm')\n            elif temp < 18: issues.append('Temp-Cold')\n            elif temp < 20: issues.append('Temp-Cool')\n\n            # --- Humidity (Based on ASHRAE 62.1 Ventilation and IAQ Standard) ---\n            hum = row['Humidity']\n            if hum > 70: issues.append('Humidity-High')\n            elif hum > 60: issues.append('Humidity-Elevated')\n            elif hum < 30: issues.append('Humidity-Low')\n\n            # --- CO2 (Based on ASHRAE 62.1 as a proxy for ventilation effectiveness) ---\n            co2 = row['CO2']\n            if co2 > 1500: issues.append('CO2-Very_Poor_Ventilation')\n            elif co2 > 1000: issues.append('CO2-Poor_Ventilation')\n            \n            # --- PM2.5 (Fine Particulates - Based on WHO/EPA Air Quality Index) ---\n            pm25 = row['PM2.5']\n            if pm25 > 55.5: issues.append('PM2.5-Very_Unhealthy')\n            elif pm25 > 35.5: issues.append('PM2.5-Unhealthy')\n            elif pm25 > 12: issues.append('PM2.5-Moderate')\n\n            # --- PM10 (Coarse Particulates - Based on WHO/EPA Air Quality Index) ---\n            pm10 = row['PM10']\n            if pm10 > 155: issues.append('PM10-Unhealthy')\n            elif pm10 > 55: issues.append('PM10-Moderate')\n\n            # --- TVOC (Total Volatile Organic Compounds - Common building science guidelines) ---\n            tvoc = row['TVOC']\n            if tvoc > 2000: issues.append('TVOC-High')\n            elif tvoc > 600: issues.append('TVOC-Elevated')\n            \n            # --- CO (Carbon Monoxide - Based on WHO/EPA exposure limits) ---\n            co = row['CO']\n            if co > 15: issues.append('CO-High') # Short-term high exposure concern\n            elif co > 9: issues.append('CO-Elevated') # Concern over 8-hour exposure\n            \n            # --- Light (Based on IES - Illuminating Engineering Society recommendations) ---\n            light = row['Light_Intensity']\n            if light > 2000: issues.append('Light-High_Glare') # Potential for glare\n            elif light < 150 and pd.to_datetime(row.name).hour in range(8, 18): # Low light during daytime\n                issues.append('Light-Low')\n\n            # --- Sustained Dampness (Microbial growth risk factor) ---\n            if row['Time_of_Wetness_72h'] > 4:\n                # Add this only if a humidity issue isn't already flagged to avoid redundancy\n                if 'Humidity-High' not in issues and 'Humidity-Elevated' not in issues:\n                    issues.append('Sustained_Dampness')\n\n            if not issues:\n                return 'Optimal'\n            else:\n                # Sort for consistency and join all identified issues into a single diagnostic label\n                return '_'.join(sorted(issues))\n\n        self.df['Holistic_State'] = self.df.apply(get_holistic_state, axis=1)\n        print(\"    Holistic environmental states defined.\")\n        print(\"\\nHolistic State Distribution (Top 20):\")\n        print(self.df['Holistic_State'].value_counts().head(20))\n\n    def _prepare_for_modeling(self):\n        \"\"\"Prepares final X, y dataframes, scales, and balances the data.\"\"\"\n        print(\"--> STAGE 4: Preparing Data for Final Model...\")\n        self.df.dropna(inplace=True)\n        self.y = self.df['Holistic_State']\n        self.X = self.df.drop(columns=['Holistic_State'])\n        self.X = pd.get_dummies(self.X, columns=['Ventilation_Status'], drop_first=True)\n        self.feature_names = self.X.columns.tolist()\n\n        self.encoder = LabelEncoder()\n        y_encoded = self.encoder.fit_transform(self.y)\n\n        if len(self.encoder.classes_) < 2:\n            raise ValueError(\"Target variable has less than 2 classes. Cannot proceed.\")\n\n        split_idx = int(len(self.X) * 0.8)\n        X_train, self.X_test = self.X.iloc[:split_idx], self.X.iloc[split_idx:]\n        y_train, self.y_test = y_encoded[:split_idx], y_encoded[split_idx:]\n\n        self.scaler = StandardScaler()\n        X_train_scaled = self.scaler.fit_transform(X_train)\n        self.X_test_scaled = self.scaler.transform(self.X_test)\n\n        unique, counts = np.unique(y_train, return_counts=True)\n        min_samples = counts.min()\n        k_neighbors = max(1, min_samples - 1)\n\n        print(f\"    Balancing training set with SMOTE (k_neighbors set to {k_neighbors})...\")\n        sm = SMOTE(random_state=SEED, k_neighbors=k_neighbors)\n        self.X_train_bal, self.y_train_bal = sm.fit_resample(X_train_scaled, y_train)\n        print(f\"    Data prepared. Training set balanced. Shape: {self.X_train_bal.shape}\")\n\n    def _tune_and_train_model(self):\n        \"\"\"Uses Hyperopt to find the best XGBoost parameters and trains the final model.\"\"\"\n        print(\"--> STAGE 5: Hyperparameter Tuning and Training...\")\n        \n        X_train_opt, X_val_opt, y_train_opt, y_val_opt = train_test_split(\n            self.X_train_bal, self.y_train_bal, test_size=0.2, random_state=SEED)\n        \n        space = {'n_estimators': hp.quniform('n_estimators', 100, 800, 50),\n                 'max_depth': hp.quniform('max_depth', 4, 15, 1),\n                 'learning_rate': hp.loguniform('learning_rate', np.log(0.01), np.log(0.2)),\n                 'gamma': hp.uniform('gamma', 0, 0.5)}\n\n        def objective(params):\n            params['n_estimators'] = int(params['n_estimators']); params['max_depth'] = int(params['max_depth'])\n            clf = XGBClassifier(**params, objective='multi:softmax', num_class=len(self.encoder.classes_),\n                                use_label_encoder=False, eval_metric='mlogloss', random_state=SEED,\n                                # --- GPU ACCELERATION ENABLED ---\n                                tree_method='gpu_hist' \n                               )\n            clf.fit(X_train_opt, y_train_opt)\n            y_pred = clf.predict(X_val_opt)\n            score = f1_score(y_val_opt, y_pred, average='weighted')\n            return {'loss': -score, 'status': STATUS_OK}\n\n        trials = Trials()\n        print(\"    Running Bayesian Optimization (Hyperopt)... (This will be much faster with a GPU)\")\n        best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials, show_progressbar=False)\n        best['n_estimators'] = int(best['n_estimators']); best['max_depth'] = int(best['max_depth'])\n        print(f\"    Best parameters found: {best}\")\n\n        # Train the final model with the best parameters, also on GPU\n        self.model = XGBClassifier(**best, objective='multi:softmax', num_class=len(self.encoder.classes_),\n                                   use_label_encoder=False, eval_metric='mlogloss', random_state=SEED,\n                                   # --- GPU ACCELERATION ENABLED ---\n                                   tree_method='gpu_hist'\n                                  )\n        self.model.fit(self.X_train_bal, self.y_train_bal)\n        print(\"    Final model trained successfully.\")\n\n    def _evaluate_and_explain(self):\n        \"\"\"Evaluates the model and generates SHAP explanations.\"\"\"\n        print(\"--> STAGE 6: Model Evaluation and SHAP Explainability...\")\n        y_pred = self.model.predict(self.X_test_scaled)\n        y_pred_labels = self.encoder.inverse_transform(y_pred)\n        y_true_labels = self.encoder.inverse_transform(self.y_test)\n\n        print(\"\\nðŸ“Š Final Model Classification Report:\\n\")\n        print(classification_report(y_true_labels, y_pred_labels, zero_division=0))\n        \n        cm = confusion_matrix(y_true_labels, y_pred_labels, labels=self.encoder.classes_)\n        plt.figure(figsize=(16, 14)); sns.heatmap(cm, annot=True, fmt='d', cmap='viridis', xticklabels=self.encoder.classes_, yticklabels=self.encoder.classes_)\n        plt.title(\"Confusion Matrix\", fontsize=16); plt.xlabel(\"Predicted\", fontsize=12); plt.ylabel(\"Actual\", fontsize=12); plt.xticks(rotation=45, ha='right'); plt.yticks(rotation=0); plt.tight_layout(); plt.show()\n\n        print(\"    Generating SHAP plots...\")\n        self.explainer = shap.TreeExplainer(self.model)\n        shap_values = self.explainer.shap_values(self.X_train_bal)\n        X_train_bal_df = pd.DataFrame(self.X_train_bal, columns=self.feature_names)\n        plt.figure(); shap.summary_plot(shap_values, X_train_bal_df, plot_type=\"bar\", max_display=20, show=True)\n        plt.figure(); shap.summary_plot(shap_values, X_train_bal_df, max_display=20, show=True)\n\n    def save_artifacts(self, model_path='holistic_diagnostic_model.joblib', scaler_path='holistic_scaler.joblib', encoder_path='holistic_encoder.joblib', features_path='holistic_features.joblib'):\n        \"\"\"Saves all artifacts required for inference.\"\"\"\n        print(\"--> STAGE 7: Saving Model Artifacts...\")\n        joblib.dump(self.model, model_path)\n        joblib.dump(self.scaler, scaler_path)\n        joblib.dump(self.encoder, encoder_path)\n        joblib.dump(self.feature_names, features_path)\n        print(f\"    Model and artifacts for holistic engine saved.\")\n\n    def run_pipeline(self):\n        \"\"\"Executes the full pipeline from data loading to saving artifacts.\"\"\"\n        self._load_and_clean_data()\n        self._engineer_features_advanced()\n        self._define_holistic_environmental_state()\n        self._prepare_for_modeling()\n        self._tune_and_train_model()\n        self._evaluate_and_explain()\n        self.save_artifacts()\n        print(\"\\nâœ…âœ…âœ… Full pipeline executed successfully! âœ…âœ…âœ…\")\n\n# ------------------------------------------------------------------------------\n# SECTION B: EXECUTION\n# ------------------------------------------------------------------------------\nif __name__ == '__main__':\n    try:\n        engine = EnvironmentalDiagnosticEngine(file_path='/kaggle/input/iot-indoor-air-quality/IoT_Indoor_Air_Quality_Dataset.csv')\n        engine.run_pipeline()\n    except FileNotFoundError:\n        print(\"\\n\\n--- FATAL ERROR: 'IoT_Indoor_Air_Quality_Dataset.csv' not found. ---\")\n    except Exception as e:\n        print(f\"\\n\\n--- AN UNEXPECTED ERROR OCCURRED ---\")\n        print(f\"Error Type: {type(e).__name__}\")\n        print(f\"Error Details: {e}\")\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# V4","metadata":{}},{"cell_type":"code","source":"!pip install --quiet --no-deps --upgrade numpy pandas scikit-learn imbalanced-learn xgboost shap seaborn plotly pywavelets matplotlib joblib","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# PROJECT: ADVANCED INDOOR MICROBIAL RISK ASSESSMENT SYSTEM (AIMRAS)\n# ------------------------------------------------------------------------------\n# SECTION A: SETUP AND ENVIRONMENT\n# ------------------------------------------------------------------------------\n\n# Core imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport shap\nimport warnings\nimport logging\nimport json\nimport os\nfrom typing import Dict, List, Tuple, Any\n\n# Advanced visualization\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n# Time series analysis\nimport pywt  # PyWavelets for wavelet transforms\n\n# Scikit-learn, Imblearn, XGBoost\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, f1_score,\n    precision_recall_curve, roc_curve, auc\n)\nfrom sklearn.ensemble import IsolationForest, RandomForestClassifier, VotingClassifier\nfrom sklearn.cluster import DBSCAN\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.combine import SMOTETomek\nfrom xgboost import XGBClassifier\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(\"aimras_analytics.log\"),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger(\"AIMRAS\")\n\n# Silence non-critical warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Set seeds for reproducibility\nSEED = 42\nnp.random.seed(SEED)\n\n# ------------------------------------------------------------------------------\n# SECTION B: CONFIGURATION SYSTEM\n# ------------------------------------------------------------------------------\n\nclass AIMRASConfig:\n    \"\"\"Simplified configuration management for the AIMRAS analytics script.\"\"\"\n    DEFAULT_CONFIG = {\n        \"data\": {\n            \"historical_file_path\": \"/kaggle/input/iot-indoor-air-quality/IoT_Indoor_Air_Quality_Dataset.csv\",\n        },\n        \"processing\": {\n            \"anomaly_contamination\": 0.01,\n            \"roll_window\": 24,\n        },\n        \"model\": {\n            \"train_test_split\": 0.8,\n            \"ensemble_models\": [\"xgboost\", \"random_forest\"],\n            \"output_dir\": \"aimras_outputs\" # Directory for saved models and graphs\n        },\n        \"microbial_thresholds\": {\n            \"mold\": {\n                \"rh_sustained\": 80.0,  # % RH\n                \"rh_duration\": 48,     # hours\n                \"temperature_range\": [5, 35],  # Â°C\n            },\n            \"virus_persistence\": {\n                \"rh_low_risk\": 20, # % RH below this increases persistence\n                \"rh_high_risk\": 80, # % RH above this increases persistence\n                \"co2_ventilation_proxy\": 1000 # ppm, high CO2 indicates poor ventilation\n            }\n        }\n    }\n\n    def __init__(self, config_file=None):\n        self.config = self.DEFAULT_CONFIG.copy()\n        if config_file and os.path.exists(config_file):\n            with open(config_file, 'r') as f:\n                file_config = json.load(f)\n                self._deep_update(self.config, file_config)\n                logger.info(f\"Loaded configuration from {config_file}\")\n        logger.info(\"Configuration initialized.\")\n\n    def _deep_update(self, d, u):\n        for k, v in u.items():\n            if isinstance(v, dict) and k in d and isinstance(d[k], dict):\n                self._deep_update(d[k], v)\n            else:\n                d[k] = v\n\n    def get(self, section, key=None):\n        if key is None:\n            return self.config.get(section, {})\n        return self.config.get(section, {}).get(key)\n\n# ------------------------------------------------------------------------------\n# SECTION C: CORE ANALYTICS ENGINE\n# ------------------------------------------------------------------------------\n\nclass EnvironmentalDiagnosticEngine:\n    \"\"\"\n    A comprehensive engine for indoor air quality analysis, featuring advanced\n    feature engineering, microbial risk assessment, and ensemble modeling.\n    \"\"\"\n    def __init__(self, config_file=None):\n        self.config = AIMRASConfig(config_file)\n        self.df = None\n        self.X_train, self.X_test = None, None\n        self.y_train, self.y_test = None, None\n        self.models = {}\n        self.scaler = None\n        self.encoder = None\n        self.feature_names = None\n        self.output_dir = self.config.get(\"model\", \"output_dir\")\n        os.makedirs(self.output_dir, exist_ok=True)\n        logger.info(f\"âœ… EnvironmentalDiagnosticEngine initialized. Outputs will be saved to '{self.output_dir}/'\")\n\n    def _load_and_clean_data(self, filepath: str):\n        \"\"\"Loads and cleans the dataset with robust validation and processing.\"\"\"\n        logger.info(\"--> STAGE 1: Loading and Cleaning Data...\")\n        try:\n            self.df = pd.read_csv(filepath)\n            logger.info(f\"Loaded {len(self.df)} rows from {filepath}\")\n\n            # Standardize column names\n            self.df.rename(columns=lambda c: c.strip().replace(' ', '_'), inplace=True)\n            \n            # Robust datetime conversion\n            self.df['Timestamp'] = pd.to_datetime(self.df['Timestamp'], errors='coerce')\n            self.df = self.df.dropna(subset=['Timestamp']).set_index('Timestamp').sort_index()\n\n            # Handle duplicates\n            self.df = self.df[~self.df.index.duplicated(keep='last')]\n\n            # Interpolate missing numeric data and forward-fill categorical\n            numeric_cols = self.df.select_dtypes(include=np.number).columns\n            self.df[numeric_cols] = self.df[numeric_cols].interpolate(method='time')\n            self.df = self.df.ffill().bfill() # Fill any remaining gaps\n\n            # Cap extreme outliers using the IQR method\n            for col in numeric_cols:\n                Q1 = self.df[col].quantile(0.25)\n                Q3 = self.df[col].quantile(0.75)\n                IQR = Q3 - Q1\n                lower_bound, upper_bound = Q1 - 2.5 * IQR, Q3 + 2.5 * IQR\n                outliers_count = ((self.df[col] < lower_bound) | (self.df[col] > upper_bound)).sum()\n                if outliers_count > 0:\n                    logger.info(f\"Capping {outliers_count} outliers in '{col}'.\")\n                    self.df[col] = self.df[col].clip(lower_bound, upper_bound)\n\n            logger.info(f\"Data cleaned successfully. Final shape: {self.df.shape}\")\n\n        except Exception as e:\n            logger.error(f\"Error in data loading/cleaning: {e}\")\n            raise\n\n    def _engineer_features(self):\n        \"\"\"Generates advanced features for temporal patterns and microbial risk.\"\"\"\n        logger.info(\"--> STAGE 2: Advanced Feature Engineering...\")\n        \n        # Time-based features\n        self.df['hour'] = self.df.index.hour\n        self.df['day_of_week'] = self.df.index.dayofweek\n        self.df['is_weekend'] = self.df['day_of_week'].isin([5, 6]).astype(int)\n        \n        # Cyclical encoding of time for models to understand time-of-day/week\n        self.df['hour_sin'] = np.sin(2 * np.pi * self.df['hour'] / 24)\n        self.df['hour_cos'] = np.cos(2 * np.pi * self.df['hour'] / 24)\n\n        # Rolling statistics for key variables\n        for col in ['Temperature', 'Humidity', 'CO2', 'PM2.5']:\n            if col in self.df.columns:\n                self.df[f'{col}_24h_mean'] = self.df[col].rolling(24, min_periods=1).mean()\n                self.df[f'{col}_24h_std'] = self.df[col].rolling(24, min_periods=1).std()\n                self.df[f'{col}_rate_of_change'] = self.df[col].diff().fillna(0)\n\n        # Microbial growth risk factors\n        mold_cfg = self.config.get(\"microbial_thresholds\", \"mold\")\n        is_wet = (self.df['Humidity'] > mold_cfg[\"rh_sustained\"]).astype(int)\n        self.df[f'time_of_wetness_{mold_cfg[\"rh_duration\"]}h'] = is_wet.rolling(window=mold_cfg[\"rh_duration\"], min_periods=1).sum()\n\n        # Dew point calculation (risk of condensation)\n        self.df['dew_point'] = self.df['Temperature'] - ((100 - self.df['Humidity']) / 5)\n\n        # Anomaly detection with Isolation Forest\n        numeric_cols = self.df.select_dtypes(include=np.number).columns\n        if_model = IsolationForest(contamination=self.config.get(\"processing\", \"anomaly_contamination\"), random_state=SEED)\n        self.df['anomaly_score'] = if_model.fit_predict(self.df[numeric_cols])\n\n        # Fill any NaNs created by rolling features\n        self.df.fillna(method='bfill', inplace=True)\n        self.df.fillna(0, inplace=True)\n        logger.info(f\"Feature engineering complete. Total features: {len(self.df.columns)}\")\n\n    def _define_holistic_environmental_state(self):\n        \"\"\"Defines a comprehensive target variable based on scientific thresholds.\"\"\"\n        logger.info(\"--> STAGE 3: Defining Holistic Environmental State (Target Variable)...\")\n        \n        mold_cfg = self.config.get(\"microbial_thresholds\", \"mold\")\n        virus_cfg = self.config.get(\"microbial_thresholds\", \"virus_persistence\")\n\n        def get_state(row):\n            issues = []\n            # Thermal Comfort (ASHRAE 55)\n            if row['Temperature'] > 28: issues.append('Temp-Hot')\n            elif row['Temperature'] < 20: issues.append('Temp-Cold')\n            \n            # Humidity (ASHRAE 62.1)\n            if row['Humidity'] > 70: issues.append('Humidity-High')\n            elif row['Humidity'] < 30: issues.append('Humidity-Low')\n\n            # Ventilation (CO2 as proxy)\n            if 'CO2' in row and row['CO2'] > 1500: issues.append('Ventilation-Poor')\n            elif 'CO2' in row and row['CO2'] > 1000: issues.append('Ventilation-Moderate')\n\n            # Particulates (WHO/EPA)\n            if 'PM2.5' in row and row['PM2.5'] > 35.5: issues.append('PM2.5-Unhealthy')\n            elif 'PM2.5' in row and row['PM2.5'] > 12: issues.append('PM2.5-Moderate')\n\n            # Mold Risk\n            is_temp_conducive = mold_cfg[\"temperature_range\"][0] <= row['Temperature'] <= mold_cfg[\"temperature_range\"][1]\n            if row[f'time_of_wetness_{mold_cfg[\"rh_duration\"]}h'] >= mold_cfg[\"rh_duration\"] and is_temp_conducive:\n                issues.append('Risk-Mold')\n\n            # Virus Persistence Risk\n            is_rh_risky = not (virus_cfg[\"rh_low_risk\"] < row['Humidity'] < virus_cfg[\"rh_high_risk\"])\n            is_vent_poor = 'CO2' in row and row['CO2'] > virus_cfg[\"co2_ventilation_proxy\"]\n            if is_rh_risky and is_vent_poor:\n                issues.append('Risk-VirusPersistence')\n\n            if not issues:\n                return 'Optimal'\n            \n            # Combine top issues for a composite state\n            if 'Risk-Mold' in issues: return 'High-Risk-Mold'\n            if 'Risk-VirusPersistence' in issues and 'Ventilation-Poor' in issues: return 'High-Risk-Airborne'\n            if len(issues) >= 3: return 'Multi-Factor-Poor'\n            \n            return sorted(issues)[0] # Return the most significant single issue\n\n        self.df['Holistic_State'] = self.df.apply(get_state, axis=1)\n        logger.info(\"Holistic environmental states defined.\")\n        logger.info(f\"\\nState Distribution:\\n{self.df['Holistic_State'].value_counts(normalize=True).to_string()}\")\n\n    def _prepare_for_modeling(self):\n        \"\"\"Prepares data for modeling, including scaling, encoding, and resampling.\"\"\"\n        logger.info(\"--> STAGE 4: Preparing Data for Modeling...\")\n        \n        self.y = self.df['Holistic_State']\n        self.X = self.df.drop(columns=['Holistic_State'])\n        self.feature_names = self.X.columns.tolist()\n\n        # Encode target variable\n        self.encoder = LabelEncoder()\n        y_encoded = self.encoder.fit_transform(self.y)\n\n        # Time-based train-test split\n        train_ratio = self.config.get(\"model\", \"train_test_split\")\n        split_idx = int(len(self.X) * train_ratio)\n        X_train, self.X_test = self.X.iloc[:split_idx], self.X.iloc[split_idx:]\n        y_train, self.y_test = y_encoded[:split_idx], y_encoded[split_idx:]\n\n        # Scale features\n        self.scaler = StandardScaler()\n        X_train_scaled = self.scaler.fit_transform(X_train)\n        self.X_test = self.scaler.transform(self.X_test) # Test data is scaled here\n\n        # Handle class imbalance using SMOTE\n        unique_classes, counts = np.unique(y_train, return_counts=True)\n        k_neighbors = max(1, min(5, np.min(counts) - 1))\n        \n        if k_neighbors > 0:\n            logger.info(f\"Handling class imbalance with SMOTE (k_neighbors={k_neighbors})...\")\n            smote = SMOTE(random_state=SEED, k_neighbors=k_neighbors)\n            self.X_train, self.y_train = smote.fit_resample(X_train_scaled, y_train)\n            logger.info(f\"Training data balanced. New shape: {self.X_train.shape}\")\n        else:\n            logger.warning(\"Cannot apply SMOTE, at least one class has fewer than 2 samples. Using original data.\")\n            self.X_train = X_train_scaled\n            self.y_train = y_train\n\n    def _train_and_save_model(self):\n        \"\"\"Trains the ensemble model and saves it along with pre-processing components.\"\"\"\n        logger.info(\"--> STAGE 5: Training and Saving Model...\")\n\n        # Define individual models\n        xgb = XGBClassifier(\n            objective='multi:softmax', num_class=len(self.encoder.classes_),\n            random_state=SEED, n_estimators=200, learning_rate=0.1, max_depth=7,\n            use_label_encoder=False, eval_metric='mlogloss'\n        )\n        rf = RandomForestClassifier(\n            n_estimators=150, max_depth=10, random_state=SEED, n_jobs=-1\n        )\n        \n        # Create the ensemble model\n        ensemble = VotingClassifier(\n            estimators=[('xgboost', xgb), ('random_forest', rf)],\n            voting='soft' # Use probabilities for better performance\n        )\n        \n        logger.info(\"Training the soft voting ensemble classifier...\")\n        ensemble.fit(self.X_train, self.y_train)\n        self.models['ensemble'] = ensemble\n        logger.info(\"Ensemble model trained successfully.\")\n\n        # Save the complete model bundle\n        model_bundle = {\n            'model': ensemble,\n            'scaler': self.scaler,\n            'encoder': self.encoder,\n            'feature_names': self.feature_names\n        }\n        model_path = os.path.join(self.output_dir, 'aimras_model_bundle.pkl')\n        joblib.dump(model_bundle, model_path)\n        logger.info(f\"Model bundle saved to: {model_path}\")\n\n\n    def _evaluate_model(self):\n        \"\"\"Evaluates the final model and generates and saves comprehensive evaluation plots.\"\"\"\n        logger.info(\"--> STAGE 6: Model Evaluation and Visualization...\")\n        \n        if 'ensemble' not in self.models:\n            logger.error(\"No ensemble model found to evaluate.\")\n            return\n\n        model = self.models['ensemble']\n        y_pred = model.predict(self.X_test)\n        y_prob = model.predict_proba(self.X_test)\n        y_true_labels = self.encoder.inverse_transform(self.y_test)\n        y_pred_labels = self.encoder.inverse_transform(y_pred)\n        class_labels = self.encoder.classes_\n        n_classes = len(class_labels)\n\n        # 1. Classification Report\n        logger.info(\"\\n--- Classification Report ---\\n\")\n        report_dict = classification_report(y_true_labels, y_pred_labels, target_names=class_labels, zero_division=0, output_dict=True)\n        logger.info(classification_report(y_true_labels, y_pred_labels, target_names=class_labels, zero_division=0))\n\n        # 2. Save Confusion Matrix\n        plt.figure(figsize=(12, 10))\n        cm = confusion_matrix(y_true_labels, y_pred_labels, labels=class_labels)\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n        plt.title(\"Ensemble Model Confusion Matrix\", fontsize=16)\n        plt.xlabel(\"Predicted State\", fontsize=12)\n        plt.ylabel(\"Actual State\", fontsize=12)\n        plt.xticks(rotation=45, ha='right'); plt.yticks(rotation=0)\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.output_dir, \"confusion_matrix.png\"))\n        plt.show()\n\n        # 3. Save SHAP Feature Importance Plot\n        logger.info(\"Generating and saving SHAP explanations...\")\n        explainer = shap.TreeExplainer(self.models['ensemble'].named_estimators_['xgboost'])\n        shap_values = explainer.shap_values(self.X_test)\n        plt.figure()\n        shap.summary_plot(shap_values, self.X_test, feature_names=self.feature_names,\n                          class_names=class_labels, plot_type=\"bar\", max_display=20, show=False)\n        plt.title(\"SHAP Feature Importance (Top 20)\", fontsize=16)\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.output_dir, \"shap_feature_importance.png\"))\n        plt.show()\n\n        # Binarize the output for PR and ROC curves\n        y_test_binarized = label_binarize(self.y_test, classes=range(n_classes))\n\n        # 4. Save Precision-Recall Curve\n        plt.figure(figsize=(12, 8))\n        for i in range(n_classes):\n            precision, recall, _ = precision_recall_curve(y_test_binarized[:, i], y_prob[:, i])\n            plt.plot(recall, precision, lw=2, label=f'PR curve for {class_labels[i]}')\n        plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n        plt.title(\"Precision-Recall Curve for Each Class\"); plt.legend(loc=\"best\")\n        plt.grid(True); plt.tight_layout()\n        plt.savefig(os.path.join(self.output_dir, \"precision_recall_curves.png\"))\n        plt.show()\n\n        # 5. Save ROC Curve\n        plt.figure(figsize=(12, 8))\n        for i in range(n_classes):\n            fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_prob[:, i])\n            roc_auc = auc(fpr, tpr)\n            plt.plot(fpr, tpr, lw=2, label=f'ROC curve for {class_labels[i]} (area = {roc_auc:0.2f})')\n        plt.plot([0, 1], [0, 1], 'k--', lw=2) # Dashed diagonal\n        plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n        plt.title(\"ROC Curve for Each Class\"); plt.legend(loc=\"lower right\")\n        plt.grid(True); plt.tight_layout()\n        plt.savefig(os.path.join(self.output_dir, \"roc_curves.png\"))\n        plt.show()\n\n        # 6. Save Overall Metrics Summary Plot\n        metrics = {\n            'Accuracy': report_dict['accuracy'],\n            'Macro Avg F1': report_dict['macro avg']['f1-score'],\n            'Weighted Avg F1': report_dict['weighted avg']['f1-score']\n        }\n        plt.figure(figsize=(8, 6))\n        sns.barplot(x=list(metrics.keys()), y=list(metrics.values()))\n        plt.title('Overall Model Performance Metrics')\n        plt.ylabel('Score')\n        plt.ylim(0, 1)\n        for index, value in enumerate(metrics.values()):\n            plt.text(index, value + 0.01, f\"{value:.3f}\", ha='center')\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.output_dir, \"metrics_summary.png\"))\n        plt.show()\n\n\n    def predict_on_new_data(self, new_data: pd.DataFrame):\n        \"\"\"Predicts the environmental state for new, unseen data using the saved model bundle.\"\"\"\n        logger.info(\"--> PREDICTION: Running inference on new data...\")\n        model_path = os.path.join(self.output_dir, 'aimras_model_bundle.pkl')\n        \n        try:\n            bundle = joblib.load(model_path)\n            model = bundle['model']\n            scaler = bundle['scaler']\n            encoder = bundle['encoder']\n            feature_names = bundle['feature_names']\n        except FileNotFoundError:\n            logger.error(f\"Model bundle not found at {model_path}. Please run the training pipeline first.\")\n            return None\n        \n        # Ensure new_data has the same columns as training data\n        new_data_processed = new_data.reindex(columns=feature_names, fill_value=0)\n        \n        # Scale the data\n        new_data_scaled = scaler.transform(new_data_processed)\n        \n        # Predict\n        predictions_encoded = model.predict(new_data_scaled)\n        predictions_proba = model.predict_proba(new_data_scaled)\n        \n        # Decode predictions to human-readable labels\n        predictions_labels = encoder.inverse_transform(predictions_encoded)\n        \n        results = []\n        for i, label in enumerate(predictions_labels):\n            confidence = predictions_proba[i][predictions_encoded[i]]\n            results.append({\"predicted_state\": label, \"confidence\": f\"{confidence:.2%}\"})\n            logger.info(f\"Prediction for sample {i}: {label} (Confidence: {confidence:.2%})\")\n            \n        return results\n\n    def run_full_pipeline(self):\n        \"\"\"Executes the entire analytics pipeline from start to finish.\"\"\"\n        logger.info(\"ðŸš€ STARTING AIMRAS FULL ANALYTICS PIPELINE ï¿½\")\n        try:\n            self._load_and_clean_data(self.config.get(\"data\", \"historical_file_path\"))\n            self._engineer_features()\n            self._define_holistic_environmental_state()\n            self._prepare_for_modeling()\n            self._train_and_save_model()\n            self._evaluate_model()\n            logger.info(\"âœ… AIMRAS FULL ANALYTICS PIPELINE COMPLETED SUCCESSFULLY âœ…\")\n        except Exception as e:\n            logger.critical(f\"Pipeline failed with a critical error: {e}\", exc_info=True)\n\n\n# ------------------------------------------------------------------------------\n# SECTION D: EXECUTION\n# ------------------------------------------------------------------------------\nif __name__ == '__main__':\n    # Initialize and run the diagnostic engine\n    engine = EnvironmentalDiagnosticEngine()\n    engine.run_full_pipeline()\n\n    # --- Example of how to use the trained model for inference ---\n    if os.path.exists(os.path.join(engine.output_dir, 'aimras_model_bundle.pkl')):\n        print(\"\\n\" + \"=\"*50)\n        print(\"DEMONSTRATING PREDICTION ON NEW (TEST) DATA\")\n        print(\"=\"*50)\n        \n        # To simulate new data, we create a dummy DataFrame.\n        # In a real scenario, you would load your new data here.\n        sample_data = {\n            'Temperature': [25.5], 'Humidity': [85.0], 'CO2': [1600], 'PM2.5': [40.0],\n            'hour': [14], 'day_of_week': [2], 'is_weekend': [0], 'hour_sin': [np.sin(2*np.pi*14/24)],\n            'hour_cos': [np.cos(2*np.pi*14/24)], 'Temperature_24h_mean': [24.8],\n            'Temperature_24h_std': [1.2], 'Temperature_rate_of_change': [0.2],\n            'Humidity_24h_mean': [82.0], 'Humidity_24h_std': [3.0], 'Humidity_rate_of_change': [1.5],\n            'CO2_24h_mean': [1450], 'CO2_24h_std': [150], 'CO2_rate_of_change': [50],\n            'PM2.5_24h_mean': [38.0], 'PM2.5_24h_std': [5.0], 'PM2.5_rate_of_change': [2.0],\n            'time_of_wetness_48h': [48], 'dew_point': [22.5], 'anomaly_score': [1]\n        }\n        sample_df = pd.DataFrame(sample_data)\n        \n        # Make predictions using the standalone function\n        predictions = engine.predict_on_new_data(sample_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-05T12:31:25.103117Z","iopub.execute_input":"2025-07-05T12:31:25.103857Z","iopub.status.idle":"2025-07-05T12:31:26.274340Z","shell.execute_reply.started":"2025-07-05T12:31:25.103822Z","shell.execute_reply":"2025-07-05T12:31:26.273152Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_130/1489315387.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_explanation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCohorts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExplanation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# explainers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexplainers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_additive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAdditiveExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/_explanation.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mslicer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAlias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mObj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSlicer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clustering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhclust_ordering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDimensionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_general\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mOpChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m from ._clustering import (\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdelta_minimization_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mhclust\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mhclust_ordering\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpartition_tree\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/shap/utils/_clustering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspatial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumba\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnjit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDimensionError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0m_ensure_critical_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;31m# END DO NOT MOVE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;31m# ---------------------- WARNING WARNING WARNING ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numba/__init__.py\u001b[0m in \u001b[0;36m_ensure_critical_deps\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         msg = (f\"Numba needs NumPy 2.0 or less. Got NumPy \"\n\u001b[1;32m     44\u001b[0m                f\"{numpy_version[0]}.{numpy_version[1]}.\")\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: Numba needs NumPy 2.0 or less. Got NumPy 2.3."],"ename":"ImportError","evalue":"Numba needs NumPy 2.0 or less. Got NumPy 2.3.","output_type":"error"}],"execution_count":1},{"cell_type":"markdown","source":"## Test V1","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\nimport joblib\nimport pandas as pd\nimport numpy as np\nimport shap\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\nclass ModelInferenceEngine:\n    \"\"\"\n    A class dedicated to loading a trained model and performing inference.\n    \"\"\"\n    def __init__(self, model_path, scaler_path, encoder_path, features_path):\n        self.model = None\n        self.scaler = None\n        self.encoder = None\n        self.feature_names = None\n        self.explainer = None\n\n        try:\n            print(\"--> Loading trained model and artifacts...\")\n            self.model = joblib.load(model_path)\n            self.scaler = joblib.load(scaler_path)\n            self.encoder = joblib.load(encoder_path)\n            self.feature_names = joblib.load(features_path)\n            self.explainer = shap.TreeExplainer(self.model)\n            print(\"    âœ… Artifacts loaded successfully.\")\n        except FileNotFoundError as e:\n            print(f\"    âŒ ERROR: Could not load a required artifact: {e}\")\n            print(\"    Please ensure the training pipeline has been run and artifacts are saved.\")\n            raise\n\n    def preprocess_input(self, input_data: dict) -> pd.DataFrame:\n        \"\"\"\n        Takes a dictionary of raw input and prepares it for the model.\n        \"\"\"\n        # Create a DataFrame from the input dictionary\n        input_df = pd.DataFrame([input_data])\n        \n        # One-hot encode ventilation status\n        input_df['Ventilation_Status_Open'] = 1 if input_data.get('Ventilation_Status') == 'Open' else 0\n        if 'Ventilation_Status' in input_df.columns:\n            input_df = input_df.drop(columns=['Ventilation_Status'])\n            \n        # Ensure all required feature columns from training exist, filling missing ones with 0\n        for col in self.feature_names:\n            if col not in input_df.columns:\n                input_df[col] = 0\n        \n        # Reorder columns to exactly match the training data order\n        return input_df[self.feature_names]\n\n    def predict(self, input_data: dict):\n        \"\"\"\n        Performs prediction and generates a human-readable explanation.\n        \"\"\"\n        if not all([self.model, self.scaler, self.encoder, self.explainer]):\n            print(\"    âŒ ERROR: Inference engine not initialized correctly. Cannot predict.\")\n            return\n\n        # 1. Preprocess the data\n        processed_df = self.preprocess_input(input_data)\n        \n        # 2. Scale the data using the loaded scaler\n        scaled_data = self.scaler.transform(processed_df)\n        \n        # 3. Make the prediction\n        prediction_encoded = self.model.predict(scaled_data)[0]\n        prediction_proba = self.model.predict_proba(scaled_data)[0]\n        prediction_label = self.encoder.inverse_transform([prediction_encoded])[0]\n        \n        # 4. Print results\n        print(\"\\n--- PREDICTION RESULT ---\")\n        print(f\"Predicted Environmental State: {prediction_label.upper()}\")\n        print(\"\\nConfidence Scores:\")\n        for i, class_name in enumerate(self.encoder.classes_):\n            print(f\"  - {class_name:20s}: {prediction_proba[i]*100:5.2f}%\")\n            \n        # 5. Generate SHAP Explanation\n        print(\"\\n--- PREDICTION EXPLANATION ---\")\n        shap_values = self.explainer.shap_values(scaled_data)\n        class_index = list(self.encoder.classes_).index(prediction_label)\n        \n        print(f\"The plot below shows the factors pushing the model's prediction towards '{prediction_label}'.\")\n        print(\"Features in RED have a positive impact (increase the score for this state).\")\n        print(\"Features in BLUE have a negative impact (decrease the score for this state).\")\n        \n        shap.initjs()\n        # Display the SHAP force plot for the predicted class\n        display(shap.force_plot(self.explainer.expected_value[class_index], \n                                shap_values[class_index], \n                                processed_df, \n                                matplotlib=True))\n\ndef run_test_scenarios():\n    \"\"\"\n    Defines and runs a series of test cases against the trained model.\n    \"\"\"\n    try:\n        engine = ModelInferenceEngine(\n            model_path='environmental_state_model.joblib',\n            scaler_path='state_scaler.joblib',\n            encoder_path='state_encoder.joblib',\n            features_path='feature_names.joblib'\n        )\n    except FileNotFoundError:\n        return # Stop if artifacts are not found\n\n    print(\"\\n\\n===================================================================\")\n    print(\"      RUNNING PREDICTION & EXPLANATION ON TEST SCENARIOS\")\n    print(\"===================================================================\")\n\n    # SCENARIO 1: Textbook \"Optimal\" conditions\n    print(\"\\n\\n--- SCENARIO 1: Optimal Conditions ---\")\n    optimal_conditions = {\n        'Temperature': 145.0, 'Humidity': 45.0, 'CO2': 550, 'PM2.5': 80, 'PM10': 1200,\n        'TVOC': 100, 'CO': 40, 'Ventilation_Status': 'Open',\n        'Time_of_Wetness_72h': 0, 'anomaly_score': 1.15 # Normal score\n    }\n    engine.predict(optimal_conditions)\n\n    # SCENARIO 2: A room with many people and closed windows -> \"Poor Ventilation\"\n    print(\"\\n\\n--- SCENARIO 2: Poor Ventilation ---\")\n    poor_ventilation = {\n        'Temperature': 24.0, 'Humidity': 62.0, 'CO2': 1800, 'PM2.5': 30, 'PM10': 45,\n        'TVOC': 500, 'CO': 9, 'Occupancy_Count': 5, 'Ventilation_Status': 'Closed',\n        'Time_of_Wetness_72h': 0, 'anomaly_score': -0.05 # Slightly anomalous due to high CO2/TVOC\n    }\n    engine.predict(poor_ventilation)\n    \n    # SCENARIO 3: A humid, stuffy room -> \"Elevated Humidity\"\n    print(\"\\n\\n--- SCENARIO 3: Elevated Humidity ---\")\n    high_humidity = {\n        'Temperature': 22.5, 'Humidity': 72.0, 'CO2': 850, 'PM2.5': 18, 'PM10': 25,\n        'TVOC': 300, 'CO': 7, 'Ventilation_Status': 'Closed',\n        'Time_of_Wetness_72h': 1, # Just started being \"wet\"\n        'anomaly_score': -0.1 \n    }\n    engine.predict(high_humidity)\n\n    # SCENARIO 4: A room after a water leak, damp for a while -> \"Sustained Dampness\"\n    print(\"\\n\\n--- SCENARIO 4: Sustained Dampness ---\")\n    sustained_dampness = {\n        'Temperature': 23.0, 'Humidity': 85.0, 'CO2': 900, 'PM2.5': 22, 'PM10': 30,\n        'TVOC': 350, 'CO': 8, 'Ventilation_Status': 'Closed',\n        'Time_of_Wetness_72h': 10, # Has been wet for 10 hours in the last 3 days\n        'anomaly_score': -0.2 # Highly anomalous\n    }\n    engine.predict(sustained_dampness)\n\n\nif __name__ == '__main__':\n    run_test_scenarios()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# âœ… Core imports\nimport joblib\nimport pandas as pd\nimport numpy as np\nimport shap\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\nclass ModelInferenceEngine:\n    \"\"\"\n    A class dedicated to loading a trained model and performing inference.\n    \"\"\"\n    def __init__(self, model_path, scaler_path, encoder_path, features_path):\n        self.model = None\n        self.scaler = None\n        self.encoder = None\n        self.feature_names = None\n        self.explainer = None\n\n        try:\n            print(\"--> Loading trained holistic model and artifacts...\")\n            self.model = joblib.load(model_path)\n            self.scaler = joblib.load(scaler_path)\n            self.encoder = joblib.load(encoder_path)\n            self.feature_names = joblib.load(features_path)\n            self.explainer = shap.TreeExplainer(self.model)\n            print(\"    âœ… Artifacts loaded successfully.\")\n        except FileNotFoundError as e:\n            print(f\"    âŒ ERROR: Could not load a required artifact: {e}\")\n            print(\"    Please ensure the training pipeline has been run and artifacts are saved.\")\n            raise\n\n    def _preprocess_input(self, input_data: dict) -> pd.DataFrame:\n        \"\"\"\n        Takes a dictionary of raw input and prepares it for the model.\n        This includes creating missing features and ensuring column order.\n        \"\"\"\n        input_df = pd.DataFrame([input_data])\n        input_df['Ventilation_Status_Open'] = 1 if input_data.get('Ventilation_Status') == 'Open' else 0\n        if 'Ventilation_Status' in input_df.columns:\n            input_df = input_df.drop(columns=['Ventilation_Status'])\n            \n        for col in self.feature_names:\n            if col not in input_df.columns:\n                input_df[col] = 0 # Default missing features to a neutral value\n        \n        return input_df[self.feature_names]\n\n    def predict(self, input_data: dict, scenario_name: str):\n        \"\"\"\n        Performs prediction for a single data point and generates a\n        human-readable explanation for the result.\n        \"\"\"\n        print(f\"\\n\\n--- SCENARIO: {scenario_name} ---\")\n        if not all([self.model, self.scaler, self.encoder, self.explainer]):\n            print(\"    âŒ ERROR: Inference engine not initialized correctly. Cannot predict.\")\n            return\n\n        processed_df = self._preprocess_input(input_data)\n        scaled_data = self.scaler.transform(processed_df)\n        \n        prediction_encoded = self.model.predict(scaled_data)[0]\n        prediction_proba = self.model.predict_proba(scaled_data)[0]\n        prediction_label = self.encoder.inverse_transform([prediction_encoded])[0]\n        \n        print(\"\\n--- PREDICTION RESULT ---\")\n        print(f\"Predicted Holistic State: {prediction_label.upper()}\")\n        print(\"\\nConfidence Scores (Top 5):\")\n        sorted_indices = np.argsort(prediction_proba)[::-1]\n        for i in sorted_indices[:5]:\n            if prediction_proba[i] > 0.01:\n                print(f\"  - {self.encoder.classes_[i]:45s}: {prediction_proba[i]*100:5.2f}%\")\n            \n        print(\"\\n--- PREDICTION EXPLANATION ---\")\n        shap_values = self.explainer.shap_values(scaled_data)\n        class_index = list(self.encoder.classes_).index(prediction_label)\n        \n        print(f\"The plot below shows the factors pushing the model's prediction towards '{prediction_label}'.\")\n        \n        shap.initjs()\n        display(shap.force_plot(self.explainer.expected_value[class_index], \n                                shap_values[class_index], \n                                processed_df, \n                                matplotlib=True))\n\ndef run_all_test_scenarios():\n    \"\"\"\n    Defines and runs a comprehensive suite of test cases to validate the model's\n    ability to diagnose different environmental conditions.\n    \"\"\"\n    try:\n        engine = ModelInferenceEngine(\n            model_path='holistic_diagnostic_model.joblib',\n            scaler_path='holistic_scaler.joblib',\n            encoder_path='holistic_encoder.joblib',\n            features_path='holistic_features.joblib'\n        )\n    except FileNotFoundError:\n        return\n\n    print(\"\\n\\n===================================================================\")\n    print(\"      RUNNING DIAGNOSTICS WITH HOLISTIC IEQ MODEL (v8.0)\")\n    print(\"===================================================================\")\n\n    engine.predict(\n        scenario_name=\"Optimal Conditions\",\n        input_data={'Temperature': 22.0, 'Humidity': 50.0, 'CO2': 600, 'PM2.5': 10, 'PM10': 20, 'TVOC': 150, 'CO': 1, 'Light_Intensity': 400}\n    )\n    \n    engine.predict(\n        scenario_name=\"Too Hot Only (Office in Summer)\",\n        input_data={'Temperature': 30.0, 'Humidity': 55.0, 'CO2': 800, 'PM2.5': 15}\n    )\n\n    engine.predict(\n        scenario_name=\"Poor Ventilation (Crowded Meeting Room)\",\n        input_data={'Temperature': 24.0, 'Humidity': 58.0, 'CO2': 1800, 'Occupancy_Count': 8}\n    )\n\n    engine.predict(\n        scenario_name=\"High Particulates (Near a Busy Road)\",\n        input_data={'Temperature': 25.0, 'Humidity': 50.0, 'CO2': 700, 'PM2.5': 40, 'PM10': 80}\n    )\n    \n    engine.predict(\n        scenario_name=\"Composite: Too Hot & Poor Ventilation\",\n        input_data={'Temperature': 29.5, 'Humidity': 59.0, 'CO2': 1600, 'Occupancy_Count': 6}\n    )\n\n    engine.predict(\n        scenario_name=\"Composite: High Humidity & High VOCs (New Paint/Furniture)\",\n        input_data={'Temperature': 23.0, 'Humidity': 68.0, 'CO2': 800, 'TVOC': 2500}\n    )\n\n    engine.predict(\n        scenario_name=\"Composite: Cold, Damp, & Stuffy (Winter, No Ventilation)\",\n        input_data={'Temperature': 17.5, 'Humidity': 72.0, 'CO2': 1300, 'Time_of_Wetness_72h': 5}\n    )\n\n    engine.predict(\n        scenario_name=\"Catastrophic Failure: All Issues Present (Post-flood, no power)\",\n        input_data={'Temperature': 28.0, 'Humidity': 85.0, 'CO2': 2500, 'PM2.5': 60, 'PM10': 100, 'TVOC': 2000, 'CO': 15, 'Time_of_Wetness_72h': 20}\n    )\n\n\nif __name__ == '__main__':\n    run_all_test_scenarios()\n","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# save output folder","metadata":{}},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\n\n# --- CONFIGURATION ---\nOUTPUT_DIR = \"/kaggle/working/\"\nOUTPUT_ZIP = \"AIMRAS.zip\"\n\n# --- ZIP THE DIRECTORY ---\nshutil.make_archive(base_name=OUTPUT_ZIP.replace(\".zip\", \"\"), format=\"zip\", root_dir=OUTPUT_DIR)\n\n# --- DISPLAY DOWNLOAD LINK ---\nprint(f\"âœ… Zipped {OUTPUT_DIR} to {OUTPUT_ZIP}\")\ndisplay(FileLink(OUTPUT_ZIP))","metadata":{"trusted":true,"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}