{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11462898,"sourceType":"datasetVersion","datasetId":7127458}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --quiet imbalanced-learn==0.10.1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# V3","metadata":{}},{"cell_type":"code","source":"# ==============================================================================\n# PROJECT: ADVANCED INDOOR MICROBIAL RISK ASSESSMENT SYSTEM (AIMRAS)\n# VERSION: 11.0 (Advanced Modeling & Hyperparameter Tuning Edition)\n#\n# KEY ENHANCEMENTS FROM PREVIOUS VERSION:\n# 1. ADVANCED ENSEMBLE MODEL: Replaced the VotingClassifier with a more powerful\n#    StackingClassifier, which uses a meta-model to learn the best way to\n#    combine predictions from base models.\n# 2. HYPERPARAMETER TUNING: Integrated GridSearchCV to automatically find the\n#    optimal parameters for the RandomForest component of the ensemble,\n#    improving model accuracy and robustness.\n# 3. ENHANCED FEATURE ENGINEERING: Added a 'temperature_x_humidity' interaction\n#    term to better capture the combined effects critical for microbial risk.\n# 4. SHAP PLOT FIX: Resolved the IndexError by using the modern shap.plots.bar()\n#    function correctly for multi-class outputs.\n# ==============================================================================\n\n# ------------------------------------------------------------------------------\n# SECTION A: SETUP AND ENVIRONMENT\n# ------------------------------------------------------------------------------\n# Core package installation with a known-stable set of pinned versions.\n# !pip install --quiet \\\n#     numpy==1.24.3 \\\n#     pandas==2.0.3 \\\n#     scikit-learn==1.3.2 \\\n#     imbalanced-learn==0.11.0 \\\n#     scipy==1.11.4 \\\n#     xgboost==2.0.3 \\\n#     shap==0.45.1 \\\n#     seaborn==0.13.2 \\\n#     matplotlib==3.8.4 \\\n#     joblib==1.4.2\n\n# Core imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport shap\nimport warnings\nimport json\nimport os\nimport re\nimport traceback\nfrom typing import Dict, List, Tuple, Any\n\n# Scikit-learn, Imblearn, XGBoost\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, label_binarize\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix,\n    precision_recall_curve, roc_curve, auc\n)\nfrom sklearn.ensemble import IsolationForest, RandomForestClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.over_sampling import SMOTE\nfrom xgboost import XGBClassifier\n\n# Silence non-critical warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Set seeds for reproducibility\nSEED = 42\nnp.random.seed(SEED)\n\n# ------------------------------------------------------------------------------\n# SECTION B: CONFIGURATION SYSTEM\n# ------------------------------------------------------------------------------\n\nclass AIMRASConfig:\n    \"\"\"Simplified configuration management for the AIMRAS analytics script.\"\"\"\n    DEFAULT_CONFIG = {\n        \"data\": {\n            \"historical_file_path\": \"/kaggle/input/iot-indoor-air-quality/IoT_Indoor_Air_Quality_Dataset.csv\",\n        },\n        \"processing\": {\n            \"anomaly_contamination\": 0.01,\n        },\n        \"model\": {\n            \"train_test_split\": 0.8,\n            \"output_dir\": \"aimras_outputs\" # Directory for saved models and graphs\n        },\n        \"microbial_thresholds\": {\n            \"mold\": {\n                \"rh_sustained\": 80.0,  # % RH\n                \"rh_duration\": 48,     # hours\n                \"temperature_range\": [5, 35],  # Â°C\n            },\n            \"virus_persistence\": {\n                \"rh_low_risk\": 20, # % RH below this increases persistence\n                \"rh_high_risk\": 80, # % RH above this increases persistence\n                \"co2_ventilation_proxy\": 1000 # ppm, high CO2 indicates poor ventilation\n            }\n        }\n    }\n\n    def __init__(self, config_file=None):\n        self.config = self.DEFAULT_CONFIG.copy()\n        if config_file and os.path.exists(config_file):\n            with open(config_file, 'r') as f:\n                file_config = json.load(f)\n                self._deep_update(self.config, file_config)\n                print(f\"Loaded configuration from {config_file}\")\n        print(\"Configuration initialized.\")\n\n    def _deep_update(self, d, u):\n        for k, v in u.items():\n            if isinstance(v, dict) and k in d and isinstance(d[k], dict):\n                self._deep_update(d[k], v)\n            else:\n                d[k] = v\n\n    def get(self, section, key=None):\n        if key is None:\n            return self.config.get(section, {})\n        return self.config.get(section, {}).get(key)\n\n# ------------------------------------------------------------------------------\n# SECTION C: CORE ANALYTICS ENGINE\n# ------------------------------------------------------------------------------\n\nclass EnvironmentalDiagnosticEngine:\n    \"\"\"\n    A comprehensive engine for indoor air quality analysis, featuring advanced\n    feature engineering, microbial risk assessment, and ensemble modeling.\n    \"\"\"\n    def __init__(self, config_file=None):\n        self.config = AIMRASConfig(config_file)\n        self.df = None\n        self.X_train, self.X_test = None, None\n        self.y_train, self.y_test = None, None\n        self.models = {}\n        self.scaler = None\n        self.encoder = None\n        self.feature_names = None\n        self.report_data = {} # To store data for the final report\n        self.output_dir = self.config.get(\"model\", \"output_dir\")\n        os.makedirs(self.output_dir, exist_ok=True)\n        print(f\"âœ… EnvironmentalDiagnosticEngine initialized. Outputs will be saved to '{self.output_dir}/'\")\n\n    def _load_and_clean_data(self, filepath: str):\n        \"\"\"Loads and cleans the dataset with robust validation and processing.\"\"\"\n        print(\"--> STAGE 1: Loading and Cleaning Data...\")\n        \n        if not os.path.exists(filepath):\n            error_message = (\n                f\"FATAL ERROR: Dataset not found at {filepath}\\n\\n\"\n                \"--- How to fix on Kaggle: ---\\n\"\n                \"1. Click '+ Add data' in the right-hand pane.\\n\"\n                \"2. Search for 'IoT Indoor Air Quality Dataset' and add it.\"\n            )\n            raise FileNotFoundError(error_message)\n        \n        try:\n            self.df = pd.read_csv(filepath)\n            print(f\"Loaded {len(self.df)} rows from {filepath}\")\n\n            def clean_col_name(column_name):\n                name = re.sub(r'\\(.*\\)', '', column_name)\n                name = name.strip().lower().replace(' ', '_').strip('_')\n                return name\n\n            self.df.rename(columns=clean_col_name, inplace=True)\n            print(f\"Standardized column names: {self.df.columns.tolist()}\")\n            \n            self.df['timestamp'] = pd.to_datetime(self.df['timestamp'], errors='coerce')\n            self.df = self.df.dropna(subset=['timestamp']).set_index('timestamp').sort_index()\n            self.df = self.df[~self.df.index.duplicated(keep='last')]\n\n            numeric_cols = self.df.select_dtypes(include=np.number).columns\n            self.df[numeric_cols] = self.df[numeric_cols].interpolate(method='time')\n            self.df = self.df.ffill().bfill()\n\n            for col in numeric_cols:\n                Q1, Q3 = self.df[col].quantile(0.25), self.df[col].quantile(0.75)\n                IQR = Q3 - Q1\n                lower, upper = Q1 - 2.5 * IQR, Q3 + 2.5 * IQR\n                if ((self.df[col] < lower) | (self.df[col] > upper)).any():\n                    print(f\"Capping outliers in '{col}'.\")\n                    self.df[col] = self.df[col].clip(lower, upper)\n\n            print(f\"Data cleaned successfully. Final shape: {self.df.shape}\")\n\n        except Exception as e:\n            print(f\"ERROR during data loading/cleaning: {e}\")\n            raise\n\n    def _generate_base_features(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Generates all features for a given dataframe, callable by train and predict.\"\"\"\n        df_out = df.copy()\n\n        # Time-based features\n        df_out['hour'] = df_out.index.hour\n        df_out['day_of_week'] = df_out.index.dayofweek\n        df_out['is_weekend'] = df_out['day_of_week'].isin([5, 6]).astype(int)\n        df_out['hour_sin'] = np.sin(2 * np.pi * df_out['hour'] / 24)\n        df_out['hour_cos'] = np.cos(2 * np.pi * df_out['hour'] / 24)\n\n        # Rolling statistics\n        for col in ['temperature', 'humidity', 'co2', 'pm2.5']:\n            if col in df_out.columns:\n                df_out[f'{col}_24h_mean'] = df_out[col].rolling(24, min_periods=1).mean()\n                df_out[f'{col}_24h_std'] = df_out[col].rolling(24, min_periods=1).std()\n                df_out[f'{col}_rate_of_change'] = df_out[col].diff()\n\n        # Microbial/physical features\n        mold_cfg = self.config.get(\"microbial_thresholds\", \"mold\")\n        df_out['temp_x_humidity'] = df_out['temperature'] * df_out['humidity']\n        is_wet = (df_out['humidity'] > mold_cfg[\"rh_sustained\"]).astype(int)\n        df_out[f'time_of_wetness_{mold_cfg[\"rh_duration\"]}h'] = is_wet.rolling(window=mold_cfg[\"rh_duration\"], min_periods=1).sum()\n        df_out['dew_point'] = df_out['temperature'] - ((100 - df_out['humidity']) / 5)\n\n        # IMPORTANT: Fill NaNs created during this process\n        df_out.fillna(method='ffill', inplace=True)\n        df_out.fillna(method='bfill', inplace=True)\n        df_out.fillna(0, inplace=True)\n\n        return df_out\n\n    def _engineer_features(self):\n        \"\"\"Wrapper for feature engineering on the main dataframe.\"\"\"\n        print(\"--> STAGE 2: Advanced Feature Engineering...\")\n        self.df = self._generate_base_features(self.df)\n\n        print(\"... Detecting anomalies with Isolation Forest...\")\n        original_numeric_cols = [\n            'temperature', 'humidity', 'co2', 'pm2.5', 'pm10',\n            'tvoc', 'co', 'light_intensity', 'occupancy_count'\n        ]\n        cols_to_use = [col for col in original_numeric_cols if col in self.df.columns]\n        \n        if_model = IsolationForest(contamination=self.config.get(\"processing\", \"anomaly_contamination\"), random_state=SEED)\n        self.df['anomaly_score'] = if_model.fit_predict(self.df[cols_to_use])\n\n        print(f\"Feature engineering complete. Total features: {len(self.df.columns)}\")\n\n    def _define_holistic_environmental_state(self):\n        \"\"\"Defines a comprehensive target variable based on scientific thresholds.\"\"\"\n        print(\"--> STAGE 3: Defining Holistic Environmental State (Target Variable)...\")\n        \n        mold_cfg = self.config.get(\"microbial_thresholds\", \"mold\")\n        virus_cfg = self.config.get(\"microbial_thresholds\", \"virus_persistence\")\n\n        def get_state(row):\n            issues = []\n            if row['temperature'] > 28: issues.append('Temp-Hot')\n            elif row['temperature'] < 20: issues.append('Temp-Cold')\n            if row['humidity'] > 70: issues.append('Humidity-High')\n            elif row['humidity'] < 30: issues.append('Humidity-Low')\n            if 'co2' in row and row['co2'] > 1500: issues.append('Ventilation-Poor')\n            elif 'co2' in row and row['co2'] > 1000: issues.append('Ventilation-Moderate')\n            if 'pm2.5' in row and row['pm2.5'] > 35.5: issues.append('PM2.5-Unhealthy')\n            elif 'pm2.5' in row and row['pm2.5'] > 12: issues.append('PM2.5-Moderate')\n\n            is_temp_conducive = mold_cfg[\"temperature_range\"][0] <= row['temperature'] <= mold_cfg[\"temperature_range\"][1]\n            if row[f'time_of_wetness_{mold_cfg[\"rh_duration\"]}h'] >= mold_cfg[\"rh_duration\"] and is_temp_conducive:\n                issues.append('Risk-Mold')\n\n            is_rh_risky = not (virus_cfg[\"rh_low_risk\"] < row['humidity'] < virus_cfg[\"rh_high_risk\"])\n            is_vent_poor = 'co2' in row and row['co2'] > virus_cfg[\"co2_ventilation_proxy\"]\n            if is_rh_risky and is_vent_poor:\n                issues.append('Risk-VirusPersistence')\n\n            if not issues: return 'Optimal'\n            return '_'.join(sorted(issues))\n\n        self.df['holistic_state'] = self.df.apply(get_state, axis=1)\n        print(\"Holistic environmental states defined.\")\n        state_distribution = self.df['holistic_state'].value_counts(normalize=True)\n        self.report_data['state_distribution'] = state_distribution\n        print(f\"\\nState Distribution (Top 15):\\n{state_distribution.head(15).to_string()}\")\n\n    def _prepare_for_modeling(self):\n        \"\"\"Prepares data for modeling, including scaling, encoding, and resampling.\"\"\"\n        print(\"--> STAGE 4: Preparing Data for Modeling...\")\n        \n        self.y = self.df['holistic_state']\n        self.X = self.df.drop(columns=['holistic_state'])\n\n        categorical_cols = self.X.select_dtypes(include=['object', 'category']).columns\n        if not categorical_cols.empty:\n            print(f\"Found categorical columns to encode: {categorical_cols.tolist()}\")\n            self.X = pd.get_dummies(self.X, columns=categorical_cols, drop_first=True)\n        \n        self.feature_names = self.X.columns.tolist()\n\n        self.encoder = LabelEncoder()\n        y_encoded = self.encoder.fit_transform(self.y)\n\n        train_ratio = self.config.get(\"model\", \"train_test_split\")\n        split_idx = int(len(self.X) * train_ratio)\n        X_train, self.X_test = self.X.iloc[:split_idx], self.X.iloc[split_idx:]\n        y_train, self.y_test = y_encoded[:split_idx], y_encoded[split_idx:]\n\n        self.scaler = StandardScaler()\n        X_train_scaled = self.scaler.fit_transform(X_train)\n        self.X_test = self.scaler.transform(self.X_test)\n\n        unique_classes, counts = np.unique(y_train, return_counts=True)\n        k_neighbors = max(1, min(5, np.min(counts) - 1))\n        \n        if k_neighbors > 0:\n            print(f\"Handling class imbalance with SMOTE (k_neighbors={k_neighbors})...\")\n            smote = SMOTE(random_state=SEED, k_neighbors=k_neighbors)\n            self.X_train, self.y_train = smote.fit_resample(X_train_scaled, y_train)\n            print(f\"Training data balanced. New shape: {self.X_train.shape}\")\n        else:\n            print(\"WARNING: Cannot apply SMOTE, at least one class has fewer than 2 samples. Using original data.\")\n            self.X_train = X_train_scaled\n            self.y_train = y_train\n\n    def _train_and_save_model(self):\n        \"\"\"Tunes, trains, and saves an advanced stacking ensemble model.\"\"\"\n        print(\"--> STAGE 5: Hyperparameter Tuning & Advanced Model Training...\")\n\n        # Define base models\n        xgb = XGBClassifier(\n            objective='multi:softmax', num_class=len(self.encoder.classes_),\n            random_state=SEED, n_estimators=200, learning_rate=0.1, max_depth=7,\n            use_label_encoder=False, eval_metric='mlogloss'\n        )\n        rf = RandomForestClassifier(random_state=SEED, n_jobs=-1)\n\n        # --- HYPERPARAMETER TUNING (DEMONSTRATION) ---\n        # In a real project, this grid would be much larger.\n        param_grid = {\n            'n_estimators': [100, 150],\n            'max_depth': [10, 20]\n        }\n        print(f\"Tuning RandomForest with GridSearchCV... (Parameter grid: {param_grid})\")\n        grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=3, n_jobs=-1, scoring='f1_weighted')\n        grid_search.fit(self.X_train, self.y_train)\n        best_rf = grid_search.best_estimator_\n        print(f\"Best RandomForest parameters found: {grid_search.best_params_}\")\n        \n        # --- ADVANCED ENSEMBLE: STACKING ---\n        estimators = [\n            ('xgboost', xgb),\n            ('random_forest', best_rf)\n        ]\n        # The meta-model learns to combine the predictions of the base models\n        meta_model = LogisticRegression(solver='liblinear')\n        \n        stacking_classifier = StackingClassifier(\n            estimators=estimators, final_estimator=meta_model, cv=5\n        )\n        \n        print(\"Training the StackingClassifier...\")\n        stacking_classifier.fit(self.X_train, self.y_train)\n        self.models['ensemble'] = stacking_classifier\n        print(\"Stacking model trained successfully.\")\n\n        # Save the complete model bundle\n        model_bundle = {\n            'model': stacking_classifier,\n            'scaler': self.scaler,\n            'encoder': self.encoder,\n            'feature_names': self.feature_names\n        }\n        model_path = os.path.join(self.output_dir, 'aimras_model_bundle.pkl')\n        joblib.dump(model_bundle, model_path)\n        print(f\"Model bundle saved to: {model_path}\")\n\n    def _evaluate_model(self):\n        \"\"\"Evaluates the final model and generates and saves comprehensive evaluation plots.\"\"\"\n        print(\"--> STAGE 6: Model Evaluation and Visualization...\")\n        \n        if 'ensemble' not in self.models:\n            print(\"ERROR: No ensemble model found to evaluate.\")\n            return\n\n        model = self.models['ensemble']\n        y_pred = model.predict(self.X_test)\n        y_prob = model.predict_proba(self.X_test)\n        y_true_labels = self.encoder.inverse_transform(self.y_test)\n        y_pred_labels = self.encoder.inverse_transform(y_pred)\n        class_labels = self.encoder.classes_\n        n_classes = len(class_labels)\n\n        # 1. Save Classification Report to file and store for final report\n        print(\"\\n--- Classification Report ---\\n\")\n        report_str = classification_report(y_true_labels, y_pred_labels, target_names=class_labels, zero_division=0)\n        print(report_str)\n        with open(os.path.join(self.output_dir, \"classification_report.txt\"), \"w\") as f:\n            f.write(report_str)\n        print(\"Classification report saved to classification_report.txt\")\n        self.report_data['classification_report'] = report_str\n        report_dict = classification_report(y_true_labels, y_pred_labels, target_names=class_labels, zero_division=0, output_dict=True)\n\n        # 2. Confusion Matrix\n        plt.figure(figsize=(12, 10))\n        cm = confusion_matrix(y_true_labels, y_pred_labels, labels=class_labels)\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_labels, yticklabels=class_labels)\n        plt.title(\"Ensemble Model Confusion Matrix\", fontsize=16)\n        plt.xlabel(\"Predicted State\", fontsize=12); plt.ylabel(\"Actual State\", fontsize=12)\n        plt.xticks(rotation=45, ha='right'); plt.yticks(rotation=0)\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.output_dir, \"confusion_matrix.png\"), dpi=300)\n        plt.show()\n\n        # 3. Model-based Feature Importance Plot\n        print(\"Generating and saving model feature importances...\")\n        # Note: StackingClassifier doesn't have a single `feature_importances_` attribute.\n        # We'll show the importances from the base XGBoost model as a proxy.\n        xgb_model = model.named_estimators_['xgboost']\n        xgb_imp = xgb_model.feature_importances_\n        imp_df = pd.DataFrame({'feature': self.feature_names, 'importance': xgb_imp})\n        imp_df = imp_df.sort_values('importance', ascending=False).head(20)\n        self.report_data['feature_importances'] = imp_df\n        \n        plt.figure(figsize=(12, 8))\n        imp_df.sort_values('importance').plot(x='feature', y='importance', kind='barh', figsize=(12, 10), legend=False)\n        plt.title('Top 20 Feature Importances (from XGBoost component)')\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.output_dir, \"feature_importances.png\"), dpi=300)\n        plt.show()\n\n        # 4. SHAP Summary Plots\n        print(\"Generating and saving SHAP explanations...\")\n        explainer = shap.TreeExplainer(xgb_model)\n        X_test_df = pd.DataFrame(self.X_test, columns=self.feature_names)\n        shap_values = explainer(X_test_df)\n\n        plt.figure(figsize=(10, 8))\n        shap.plots.bar(shap_values, max_display=20, show=False)\n        plt.title(\"SHAP Feature Importance (Mean Absolute SHAP Value)\", fontsize=16)\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.output_dir, \"shap_bar_plot.png\"), dpi=300)\n        plt.show()\n\n        plt.figure(figsize=(10, 8))\n        shap.summary_plot(shap_values, X_test_df, max_display=20, show=False)\n        plt.title(\"SHAP Feature Impact Summary\", fontsize=16)\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.output_dir, \"shap_beeswarm_plot.png\"), dpi=300)\n        plt.show()\n\n        # Binarize the output for PR and ROC curves\n        y_test_binarized = label_binarize(self.y_test, classes=range(n_classes))\n\n        # 5. Precision-Recall and ROC Curves\n        if n_classes <= 15:\n            plt.figure(figsize=(12, 8))\n            for i in range(n_classes):\n                precision, recall, _ = precision_recall_curve(y_test_binarized[:, i], y_prob[:, i])\n                plt.plot(recall, precision, lw=2, label=f'PR curve for {class_labels[i]}')\n            plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n            plt.title(\"Precision-Recall Curve for Each Class\"); plt.legend(loc=\"best\")\n            plt.grid(True); plt.tight_layout()\n            plt.savefig(os.path.join(self.output_dir, \"precision_recall_curves.png\"), dpi=300)\n            plt.show()\n\n            plt.figure(figsize=(12, 8))\n            for i in range(n_classes):\n                fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_prob[:, i])\n                roc_auc = auc(fpr, tpr)\n                plt.plot(fpr, tpr, lw=2, label=f'ROC curve for {class_labels[i]} (area = {roc_auc:0.2f})')\n            plt.plot([0, 1], [0, 1], 'k--', lw=2)\n            plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n            plt.title(\"ROC Curve for Each Class\"); plt.legend(loc=\"lower right\")\n            plt.grid(True); plt.tight_layout()\n            plt.savefig(os.path.join(self.output_dir, \"roc_curves.png\"), dpi=300)\n            plt.show()\n        else:\n            print(\"Skipping PR and ROC curves due to high number of classes, which would make the plot unreadable.\")\n\n        # 6. Overall Metrics Summary Plot\n        metrics = {'Accuracy': report_dict['accuracy'], 'Macro Avg F1': report_dict['macro avg']['f1-score'], 'Weighted Avg F1': report_dict['weighted avg']['f1-score']}\n        plt.figure(figsize=(8, 6))\n        sns.barplot(x=list(metrics.keys()), y=list(metrics.values()))\n        plt.title('Overall Model Performance Metrics'); plt.ylabel('Score'); plt.ylim(0, 1)\n        for index, value in enumerate(metrics.values()):\n            plt.text(index, value + 0.01, f\"{value:.3f}\", ha='center')\n        plt.tight_layout()\n        plt.savefig(os.path.join(self.output_dir, \"metrics_summary.png\"), dpi=300)\n        plt.show()\n\n    def _generate_research_paper_report(self):\n        \"\"\"Generates a detailed text report suitable for a research paper.\"\"\"\n        print(\"--> STAGE 7: Generating Research Paper Report...\")\n        \n        report_path = os.path.join(self.output_dir, \"research_report.txt\")\n        \n        with open(report_path, \"w\") as f:\n            f.write(\"=\"*80 + \"\\n\")\n            f.write(\"AIMRAS: ADVANCED INDOOR MICROBIAL RISK ASSESSMENT SYSTEM - RESULTS\\n\")\n            f.write(\"=\"*80 + \"\\n\\n\")\n\n            # Section 1: Data Summary\n            f.write(\"1. DATA SUMMARY\\n\")\n            f.write(\"-\"*80 + \"\\n\")\n            f.write(f\"The analysis was conducted on the 'IoT Indoor Air Quality Dataset'.\\n\")\n            f.write(f\"Total observations after cleaning: {self.df.shape[0]}\\n\")\n            f.write(f\"Total features after engineering: {self.df.shape[1] - 1}\\n\\n\")\n            f.write(\"Distribution of Environmental States (Target Variable):\\n\")\n            f.write(self.report_data['state_distribution'].to_string())\n            f.write(\"\\n\\n\")\n\n            # Section 2: Model Performance\n            f.write(\"2. MODEL PERFORMANCE\\n\")\n            f.write(\"-\"*80 + \"\\n\")\n            f.write(\"An advanced StackingClassifier ensemble model was trained. This model uses a Logistic Regression\\n\")\n            f.write(\"meta-model to intelligently combine predictions from two base models: XGBoost and a tuned RandomForest.\\n\")\n            f.write(\"Hyperparameter tuning (GridSearchCV) was performed on the RandomForest component.\\n\\n\")\n            f.write(\"The following metrics were achieved on the unseen test set:\\n\\n\")\n            f.write(self.report_data['classification_report'])\n            f.write(\"\\n\\n\")\n\n            # Section 3: Feature Importance\n            f.write(\"3. FEATURE IMPORTANCE ANALYSIS\\n\")\n            f.write(\"-\"*80 + \"\\n\")\n            f.write(\"The top 10 most influential features from the XGBoost component of the stack were:\\n\")\n            top_10_features = self.report_data['feature_importances'].head(10)\n            f.write(top_10_features[['feature', 'importance']].to_string(index=False))\n            f.write(\"\\n\\n\")\n            \n            # Conclusion\n            f.write(\"4. CONCLUSION\\n\")\n            f.write(\"-\"*80 + \"\\n\")\n            f.write(\"The advanced stacking ensemble model demonstrated high performance in classifying holistic environmental states.\\n\")\n            f.write(\"Key drivers for prediction included both raw sensor values (e.g., pm2.5) and engineered temporal features (e.g., hour_sin).\\n\")\n            f.write(\"This indicates that both immediate conditions and time-based patterns are crucial for accurate assessment.\\n\")\n            f.write(\"All supporting graphs (Confusion Matrix, Feature Importance, SHAP plots, etc.) are saved in the output directory.\\n\")\n\n        print(f\"âœ… Research report saved to: {report_path}\")\n\n    def predict_on_new_data(self, new_data: pd.DataFrame):\n        \"\"\"Predicts the environmental state for new, unseen data using the saved model bundle.\"\"\"\n        print(\"--> PREDICTION: Running inference on new data...\")\n        model_path = os.path.join(self.output_dir, 'aimras_model_bundle.pkl')\n        \n        try:\n            bundle = joblib.load(model_path)\n            model, scaler, encoder, feature_names = bundle['model'], bundle['scaler'], bundle['encoder'], bundle['feature_names']\n        except FileNotFoundError:\n            print(f\"ERROR: Model bundle not found at {model_path}. Please run the training pipeline first.\")\n            return None\n        \n        # Ensure index is datetime for feature generation\n        if 'timestamp' in new_data.columns:\n            new_data['timestamp'] = pd.to_datetime(new_data['timestamp'])\n            new_data = new_data.set_index('timestamp')\n        else:\n            new_data.index = [pd.to_datetime(pd.Timestamp.now())]\n\n        # Generate the same features as the training pipeline\n        new_data_featured = self._generate_base_features(new_data)\n        \n        # Add anomaly_score feature (assuming new data is not an anomaly)\n        new_data_featured['anomaly_score'] = 1\n        \n        # Handle categorical columns in the new data\n        categorical_cols = new_data_featured.select_dtypes(include=['object', 'category']).columns\n        if not categorical_cols.empty:\n            new_data_featured = pd.get_dummies(new_data_featured, columns=categorical_cols, drop_first=True)\n\n        new_data_processed = new_data_featured.reindex(columns=feature_names, fill_value=0)\n        new_data_scaled = scaler.transform(new_data_processed)\n        \n        predictions_encoded = model.predict(new_data_scaled)\n        predictions_proba = model.predict_proba(new_data_scaled)\n        predictions_labels = encoder.inverse_transform(predictions_encoded)\n        \n        results = []\n        for i, label in enumerate(predictions_labels):\n            confidence = predictions_proba[i][predictions_encoded[i]]\n            results.append({\"predicted_state\": label, \"confidence\": f\"{confidence:.2%}\"})\n            print(f\"Prediction for sample {i}: {label} (Confidence: {confidence:.2%})\")\n            \n        return results\n\n    def run_full_pipeline(self):\n        \"\"\"Executes the entire analytics pipeline from start to finish.\"\"\"\n        print(\"ðŸš€ STARTING AIMRAS FULL ANALYTICS PIPELINE ðŸš€\")\n        try:\n            # --- Main Pipeline ---\n            stages = [\n                (\"Loading and Cleaning Data\", self._load_and_clean_data, self.config.get(\"data\", \"historical_file_path\")),\n                (\"Engineering Features\", self._engineer_features, None),\n                (\"Defining Holistic Environmental State\", self._define_holistic_environmental_state, None),\n                (\"Preparing Data for Modeling\", self._prepare_for_modeling, None),\n                (\"Training and Saving Model\", self._train_and_save_model, None),\n                (\"Evaluating Model\", self._evaluate_model, None),\n                (\"Generating Research Paper Report\", self._generate_research_paper_report, None)\n            ]\n\n            for i, (name, func, arg) in enumerate(stages):\n                print(f\"\\n[STAGE {i+1}/{len(stages)}] {name}...\")\n                if arg:\n                    func(arg)\n                else:\n                    func()\n                print(f\"âœ… Stage {i+1} Complete\")\n\n            print(\"\\n\\nðŸŽ‰ AIMRAS FULL ANALYTICS PIPELINE COMPLETED SUCCESSFULLY! ðŸŽ‰\")\n            print(f\"All outputs saved in the '{self.output_dir}' directory.\")\n\n        except Exception:\n            print(f\"\\nâŒ PIPELINE FAILED! See the error traceback below.\")\n            traceback.print_exc()\n\n# ------------------------------------------------------------------------------\n# SECTION D: EXECUTION\n# ------------------------------------------------------------------------------\nif __name__ == '__main__':\n    engine = EnvironmentalDiagnosticEngine()\n    engine.run_full_pipeline()\n\n    if os.path.exists(os.path.join(engine.output_dir, 'aimras_model_bundle.pkl')):\n        print(\"\\n\" + \"=\"*50)\n        print(\"DEMONSTRATING PREDICTION ON NEW (TEST) DATA\")\n        print(\"=\"*50)\n        \n        # To simulate new data, we create a dummy DataFrame with only raw inputs.\n        sample_data = {\n            'timestamp': [pd.Timestamp.now()],\n            'temperature': [25.5], \n            'humidity': [85.0], \n            'co2': [1600], \n            'pm2.5': [40.0],\n            'motion_detected': [True], \n            'ventilation_status': ['Open'] \n        }\n        sample_df = pd.DataFrame(sample_data)\n        \n        # The prediction function will handle all feature engineering and encoding\n        predictions = engine.predict_on_new_data(sample_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nfrom IPython.display import FileLink\n\n# --- CONFIGURATION ---\nOUTPUT_DIR = \"/kaggle/working/\"\nOUTPUT_ZIP = \"AIMRAS V13.zip\"\n\n# --- ZIP THE DIRECTORY ---\nshutil.make_archive(base_name=OUTPUT_ZIP.replace(\".zip\", \"\"), format=\"zip\", root_dir=OUTPUT_DIR)\n\n# --- DISPLAY DOWNLOAD LINK ---\nprint(f\"âœ… Zipped {OUTPUT_DIR} to {OUTPUT_ZIP}\")\ndisplay(FileLink(OUTPUT_ZIP))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install --quiet \\\n     numpy==1.24.3 \\\n     pandas==2.0.3 \\\n     scikit-learn==1.3.2 \\\n     scipy==1.11.4 \\\n     xgboost==2.0.3 \\\n     shap==0.45.1 \\\n     seaborn==0.13.2 \\\n     matplotlib==3.8.4 \\\n     joblib==1.4.2","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# PROJECT: ADVANCED INDOOR MICROBIAL RISK ASSESSMENT SYSTEM (AIMRAS)\n# VERSION: 11.2 (Robust Training Engine)\n#\n# KEY ENHANCEMENTS FROM PREVIOUS VERSION:\n# 1. XGBOOST ERROR FIX:\n#    - Resolved the `xgboost.core.XGBoostError` by removing the invalid parameter\n#      `scale_pos_weight='auto'`. The model now uses its default robust\n#      handling for class weights.\n#\n# 2. AUTOMATIC LABEL PRUNING (CRITICAL FIX):\n#    - Implemented a pre-training check to identify and exclude target labels\n#      that have no class variation (i.e., all 0s or all 1s).\n#    - This prevents the model from crashing when encountering data where certain\n#      risks never occur, making the pipeline significantly more robust.\n#\n# 3. ENHANCED LOGGING:\n#    - Added clear print statements to inform the user which labels were\n#      pruned due to a lack of data, improving transparency.\n# ==============================================================================\n\n# ------------------------------------------------------------------------------\n# SECTION A: SETUP AND ENVIRONMENT\n# ------------------------------------------------------------------------------\n# Core package installation with a known-stable set of pinned versions.\n# !pip install --quiet \\\n#      numpy==1.24.3 \\\n#      pandas==2.0.3 \\\n#      scikit-learn==1.3.2 \\\n#      scipy==1.11.4 \\\n#      xgboost==2.0.3 \\\n#      shap==0.45.1 \\\n#      seaborn==0.13.2 \\\n#      matplotlib==3.8.4 \\\n#      joblib==1.4.2\n\n# Core imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport shap\nimport warnings\nimport json\nimport os\nimport re\nimport traceback\nfrom typing import Dict, List, Tuple, Any\n\n# Scikit-learn, XGBoost\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import (\n    classification_report, confusion_matrix, jaccard_score, hamming_loss\n)\nfrom sklearn.multioutput import ClassifierChain\nfrom xgboost import XGBClassifier\n\n# Silence non-critical warnings\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Set seeds for reproducibility\nSEED = 42\nnp.random.seed(SEED)\n\n# ------------------------------------------------------------------------------\n# SECTION B: CONFIGURATION SYSTEM\n# ------------------------------------------------------------------------------\n\nclass AIMRASConfig:\n    \"\"\"Manages configuration for the AIMRAS analytics script.\"\"\"\n    DEFAULT_CONFIG = {\n        \"data\": {\n            \"historical_file_path\": \"/kaggle/input/iot-indoor-air-quality/IoT_Indoor_Air_Quality_Dataset.csv\",\n        },\n        \"processing\": {\n             \"outlier_capping_factor\": 2.5 # IQR multiplier for capping outliers\n        },\n        \"model\": {\n            \"train_test_split\": 0.8,\n            \"output_dir\": \"aimras_outputs_v11\" # Directory for saved models and graphs\n        },\n        \"risk_thresholds\": {\n            \"mold_growth\": {\n                \"rh_sustained\": 75.0,  # % RH - Lowered for more sensitivity\n                \"rh_duration\": 48,      # hours\n                \"temperature_range\": [5, 35],  # Â°C\n            },\n            \"virus_persistence\": {\n                \"rh_optimal_range\": [40, 60], # % RH range where persistence is lowest\n                \"co2_ventilation_proxy\": 1200 # ppm, high CO2 indicates poor ventilation\n            },\n            \"particulate_matter\": {\n                \"pm2.5_high\": 35.4, # Unhealthy for sensitive groups\n                \"pm2.5_moderate\": 12.0\n            },\n            \"ventilation\": {\n                \"co2_poor\": 1500, # ppm\n                \"co2_moderate\": 1000 # ppm\n            },\n            \"thermal_comfort\": {\n                \"temp_hot\": 27.0, # Â°C\n                \"temp_cold\": 20.0 # Â°C\n            },\n            \"humidity_comfort\": {\n                \"rh_high\": 65.0, # %\n                \"rh_low\": 30.0 # %\n            }\n        }\n    }\n\n    def __init__(self, config_file=None):\n        self.config = self.DEFAULT_CONFIG.copy()\n        if config_file and os.path.exists(config_file):\n            with open(config_file, 'r') as f:\n                file_config = json.load(f)\n                self._deep_update(self.config, file_config)\n            print(f\"Loaded configuration from {config_file}\")\n        print(\"Configuration initialized.\")\n\n    def _deep_update(self, d, u):\n        for k, v in u.items():\n            if isinstance(v, dict) and k in d and isinstance(d[k], dict):\n                self._deep_update(d[k], v)\n            else:\n                d[k] = v\n\n    def get(self, section, key=None):\n        if key is None:\n            return self.config.get(section, {})\n        return self.config.get(section, {}).get(key)\n\n# ------------------------------------------------------------------------------\n# SECTION C: CORE ANALYTICS ENGINE\n# ------------------------------------------------------------------------------\n\nclass EnvironmentalDiagnosticEngine:\n    \"\"\"\n    A comprehensive engine for indoor air quality analysis, featuring a multi-label\n    diagnostic approach to identify specific environmental risks.\n    \"\"\"\n    def __init__(self, config_file=None):\n        self.config = AIMRASConfig(config_file)\n        self.df = None\n        self.X_train, self.X_test = None, None\n        self.y_train, self.y_test = None, None\n        self.model = None\n        self.scaler = None\n        self.feature_names = None\n        self.target_labels = None\n        self.report_data = {}\n        self.output_dir = self.config.get(\"model\", \"output_dir\")\n        os.makedirs(self.output_dir, exist_ok=True)\n        print(f\"âœ… EnvironmentalDiagnosticEngine initialized. Outputs will be saved to '{self.output_dir}/'\")\n\n    def _load_and_clean_data(self, filepath: str):\n        \"\"\"Loads and cleans the dataset with robust validation and processing.\"\"\"\n        print(\"--> STAGE 1: Loading and Cleaning Data...\")\n\n        if not os.path.exists(filepath):\n            error_message = (\n                f\"FATAL ERROR: Dataset not found at {filepath}\\n\\n\"\n                \"--- How to fix on Kaggle: ---\\n\"\n                \"1. Click '+ Add data' in the right-hand pane.\\n\"\n                \"2. Search for 'IoT Indoor Air Quality Dataset' and add it.\"\n            )\n            raise FileNotFoundError(error_message)\n\n        try:\n            self.df = pd.read_csv(filepath)\n            print(f\"Loaded {len(self.df)} rows from {filepath}\")\n\n            def clean_col_name(column_name):\n                name = re.sub(r'\\(.*\\)', '', column_name)\n                name = name.strip().lower().replace(' ', '_').replace('-', '_')\n                return name\n\n            self.df.rename(columns=clean_col_name, inplace=True)\n            print(f\"Standardized column names: {self.df.columns.tolist()}\")\n\n            self.df['timestamp'] = pd.to_datetime(self.df['timestamp'], errors='coerce')\n            self.df = self.df.dropna(subset=['timestamp']).set_index('timestamp').sort_index()\n            self.df = self.df[~self.df.index.duplicated(keep='last')]\n\n            numeric_cols = self.df.select_dtypes(include=np.number).columns\n            self.df[numeric_cols] = self.df[numeric_cols].interpolate(method='time')\n            self.df = self.df.ffill().bfill()\n\n            capping_factor = self.config.get(\"processing\", \"outlier_capping_factor\")\n            for col in numeric_cols:\n                Q1, Q3 = self.df[col].quantile(0.25), self.df[col].quantile(0.75)\n                IQR = Q3 - Q1\n                lower, upper = Q1 - capping_factor * IQR, Q3 + capping_factor * IQR\n                if ((self.df[col] < lower) | (self.df[col] > upper)).any():\n                    print(f\"Capping outliers in '{col}'.\")\n                    self.df[col] = self.df[col].clip(lower, upper)\n\n            print(f\"Data cleaned successfully. Final shape: {self.df.shape}\")\n\n        except Exception as e:\n            print(f\"ERROR during data loading/cleaning: {e}\")\n            raise\n\n    def _engineer_features(self):\n        \"\"\"Generates advanced features for time-series and interaction analysis.\"\"\"\n        print(\"--> STAGE 2: Advanced Feature Engineering...\")\n        df_out = self.df.copy()\n\n        # Time-based features\n        df_out['hour'] = df_out.index.hour\n        df_out['day_of_week'] = df_out.index.dayofweek\n        df_out['is_weekend'] = df_out['day_of_week'].isin([5, 6]).astype(int)\n        df_out['hour_sin'] = np.sin(2 * np.pi * df_out['hour'] / 24)\n        df_out['hour_cos'] = np.cos(2 * np.pi * df_out['hour'] / 24)\n\n        # Rolling statistics & Lag features\n        key_sensors = ['temperature', 'humidity', 'co2', 'pm2.5']\n        for col in key_sensors:\n            if col in df_out.columns:\n                df_out[f'{col}_24h_mean'] = df_out[col].rolling('24H', min_periods=1).mean()\n                df_out[f'{col}_1h_lag'] = df_out[col].shift(1)\n                df_out[f'{col}_rate_of_change'] = df_out[col].diff()\n\n        # Interaction & Physical Features\n        df_out['temp_humidity_interaction'] = df_out['temperature'] * df_out['humidity']\n        df_out['dew_point'] = df_out['temperature'] - ((100 - df_out['humidity']) / 5)\n        df_out['dew_point_spread'] = df_out['temperature'] - df_out['dew_point']\n\n        # Fill NaNs created during feature engineering\n        df_out = df_out.ffill().bfill()\n        df_out.fillna(0, inplace=True)\n\n        self.df = df_out\n        print(f\"Feature engineering complete. Total features: {len(self.df.columns)}\")\n\n    def _define_risk_and_comfort_labels(self):\n        \"\"\"Defines a comprehensive suite of binary risk and comfort labels for multi-label classification.\"\"\"\n        print(\"--> STAGE 3: Defining Comprehensive Risk & Comfort States (Targets)...\")\n        \n        thresholds = self.config.get(\"risk_thresholds\")\n        df = self.df\n\n        # Risk 1: Mold Growth Potential\n        mold_cfg = thresholds['mold_growth']\n        is_wet = (df['humidity'] > mold_cfg[\"rh_sustained\"]).astype(int)\n        time_of_wetness = is_wet.rolling(window=f'{mold_cfg[\"rh_duration\"]}H').sum()\n        is_temp_conducive = df['temperature'].between(mold_cfg[\"temperature_range\"][0], mold_cfg[\"temperature_range\"][1])\n        df['Risk_Mold_Growth'] = ((time_of_wetness >= mold_cfg[\"rh_duration\"]) & is_temp_conducive).astype(int)\n\n        # Risk 2: Airborne Virus Persistence\n        virus_cfg = thresholds['virus_persistence']\n        is_rh_risky = ~df['humidity'].between(virus_cfg[\"rh_optimal_range\"][0], virus_cfg[\"rh_optimal_range\"][1])\n        is_vent_poor_for_virus = df['co2'] > virus_cfg[\"co2_ventilation_proxy\"]\n        df['Risk_Virus_Persistence'] = (is_rh_risky & is_vent_poor_for_virus).astype(int)\n\n        # Risk 3 & 4: Particulate Matter (High and Moderate)\n        pm_cfg = thresholds['particulate_matter']\n        df['Risk_High_PM2.5'] = (df['pm2.5'] > pm_cfg['pm2.5_high']).astype(int)\n        df['Risk_Moderate_PM2.5'] = (df['pm2.5'].between(pm_cfg['pm2.5_moderate'], pm_cfg['pm2.5_high'])).astype(int)\n\n        # Risk 5 & 6: Ventilation (Poor and Moderate)\n        vent_cfg = thresholds['ventilation']\n        df['Risk_Poor_Ventilation'] = (df['co2'] > vent_cfg['co2_poor']).astype(int)\n        df['Risk_Moderate_Ventilation'] = (df['co2'].between(vent_cfg['co2_moderate'], vent_cfg['co2_poor'])).astype(int)\n\n        # Comfort 7 & 8: Thermal Comfort\n        thermal_cfg = thresholds['thermal_comfort']\n        df['Comfort_Thermal_Hot'] = (df['temperature'] > thermal_cfg['temp_hot']).astype(int)\n        df['Comfort_Thermal_Cold'] = (df['temperature'] < thermal_cfg['temp_cold']).astype(int)\n\n        # Comfort 9 & 10: Humidity Comfort\n        humidity_cfg = thresholds['humidity_comfort']\n        df['Comfort_Humidity_High'] = (df['humidity'] > humidity_cfg['rh_high']).astype(int)\n        df['Comfort_Humidity_Low'] = (df['humidity'] < humidity_cfg['rh_low']).astype(int)\n\n        all_labels = [\n            'Risk_Mold_Growth', 'Risk_Virus_Persistence',\n            'Risk_High_PM2.5', 'Risk_Moderate_PM2.5',\n            'Risk_Poor_Ventilation', 'Risk_Moderate_Ventilation',\n            'Comfort_Thermal_Hot', 'Comfort_Thermal_Cold',\n            'Comfort_Humidity_High', 'Comfort_Humidity_Low'\n        ]\n        self.df.dropna(subset=all_labels, inplace=True)\n\n        print(\"Comprehensive labels defined. Initial distribution of states:\")\n        distribution = self.df[all_labels].sum() / len(self.df)\n        self.report_data['initial_risk_distribution'] = distribution\n        print(distribution)\n\n        # --- ROBUSTNESS FIX: Prune labels with no variation ---\n        self.target_labels = []\n        for label in all_labels:\n            if self.df[label].nunique() > 1:\n                self.target_labels.append(label)\n            else:\n                print(f\"WARNING: Pruning label '{label}' due to no variation (all values are the same).\")\n        \n        print(f\"\\nFinal set of {len(self.target_labels)} trainable labels: {self.target_labels}\")\n\n\n    def _prepare_for_modeling(self):\n        \"\"\"Prepares data for multi-label modeling.\"\"\"\n        print(\"--> STAGE 4: Preparing Data for Modeling...\")\n\n        # Drop original sensor columns that are now represented by lags/means\n        # and categorical columns before defining X\n        cols_to_drop_for_X = ['ventilation_status']\n        self.df.drop(columns=[col for col in cols_to_drop_for_X if col in self.df.columns], inplace=True)\n\n        self.y = self.df[self.target_labels]\n        self.X = self.df.drop(columns=self.target_labels)\n        \n        # Align columns - crucial for prediction\n        self.feature_names = self.X.columns.tolist()\n\n        train_ratio = self.config.get(\"model\", \"train_test_split\")\n        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n            self.X, self.y, train_size=train_ratio, shuffle=False # Time-series split\n        )\n\n        self.scaler = StandardScaler()\n        self.X_train = self.scaler.fit_transform(self.X_train)\n        self.X_test = self.scaler.transform(self.X_test)\n        \n        print(f\"Data prepared. Train shape: {self.X_train.shape}, Test shape: {self.X_test.shape}\")\n\n    def _train_and_save_model(self):\n        \"\"\"Trains a ClassifierChain model for multi-label prediction.\"\"\"\n        print(\"--> STAGE 5: Training and Saving Model...\")\n\n        # --- FIX: Removed `scale_pos_weight='auto'` as it's an invalid parameter ---\n        # XGBoost's default behavior is generally robust. For highly imbalanced data,\n        # other strategies like over/under-sampling or explicitly calculating\n        # weights would be needed, but this fix resolves the crash.\n        base_classifier = XGBClassifier(\n            objective='binary:logistic',\n            random_state=SEED,\n            n_estimators=150,\n            learning_rate=0.05,\n            max_depth=5,\n            use_label_encoder=False,\n            eval_metric='logloss'\n        )\n\n        # Use ClassifierChain to model label dependencies\n        self.model = ClassifierChain(base_classifier, order='random', random_state=SEED)\n\n        print(\"Training the ClassifierChain with XGBoost base estimator...\")\n        self.model.fit(self.X_train, self.y_train)\n        print(\"Model trained successfully.\")\n\n        model_bundle = {\n            'model': self.model,\n            'scaler': self.scaler,\n            'feature_names': self.feature_names,\n            'target_labels': self.target_labels\n        }\n        model_path = os.path.join(self.output_dir, 'aimras_model_bundle.pkl')\n        joblib.dump(model_bundle, model_path)\n        print(f\"Model bundle saved to: {model_path}\")\n\n    def _evaluate_model(self):\n        \"\"\"Evaluates the multi-label model and generates per-risk explanations.\"\"\"\n        print(\"--> STAGE 6: Model Evaluation and Visualization...\")\n        \n        if not self.model:\n            print(\"ERROR: No model found to evaluate.\")\n            return\n\n        y_pred = self.model.predict(self.X_test)\n        y_prob = self.model.predict_proba(self.X_test)\n\n        # Convert predictions to DataFrame for easier handling\n        y_pred_df = pd.DataFrame(y_pred, columns=self.target_labels, index=self.y_test.index)\n\n        # 1. Overall Multi-Label Metrics\n        print(\"\\n--- Overall Multi-Label Metrics ---\")\n        j_score = jaccard_score(self.y_test, y_pred_df, average='samples')\n        h_loss = hamming_loss(self.y_test, y_pred_df)\n        print(f\"Sample-Averaged Jaccard Score: {j_score:.4f}\")\n        print(f\"Hamming Loss (fraction of wrong labels): {h_loss:.4f}\")\n        self.report_data['overall_metrics'] = {'jaccard_score': j_score, 'hamming_loss': h_loss}\n\n        # 2. Per-Label Evaluation and SHAP Analysis\n        X_test_df = pd.DataFrame(self.X_test, columns=self.feature_names)\n\n        for i, label in enumerate(self.target_labels):\n            print(f\"\\n--- Evaluating and Explaining: {label} ---\")\n            \n            # A. Classification Report and Confusion Matrix\n            report_str = classification_report(self.y_test[label], y_pred_df[label], zero_division=0)\n            print(report_str)\n            self.report_data[f'report_{label}'] = report_str\n\n            plt.figure(figsize=(6, 5))\n            cm = confusion_matrix(self.y_test[label], y_pred_df[label])\n            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No', 'Yes'], yticklabels=['No', 'Yes'])\n            plt.title(f\"Confusion Matrix for {label}\", fontsize=14)\n            plt.xlabel(\"Predicted\"); plt.ylabel(\"Actual\")\n            plt.tight_layout()\n            plt.savefig(os.path.join(self.output_dir, f\"confusion_matrix_{label}.png\"), dpi=300)\n            plt.show()\n\n            # B. SHAP Analysis for this specific risk\n            print(f\"Generating SHAP explanations for {label}...\")\n            estimator = self.model.estimators_[i]\n            explainer = shap.TreeExplainer(estimator)\n            \n            shap_values = explainer(X_test_df)\n\n            # SHAP Bar Plot (Global Importance)\n            plt.figure()\n            shap.plots.bar(shap_values, max_display=15, show=False)\n            plt.title(f\"Feature Importance for {label}\", fontsize=14)\n            plt.tight_layout()\n            plt.savefig(os.path.join(self.output_dir, f\"shap_bar_plot_{label}.png\"), dpi=300)\n            plt.show()\n\n            # SHAP Beeswarm Plot (Detailed Impact)\n            plt.figure()\n            shap.summary_plot(shap_values, X_test_df, max_display=15, show=False)\n            plt.title(f\"SHAP Feature Impact on {label}\", fontsize=14)\n            plt.tight_layout()\n            plt.savefig(os.path.join(self.output_dir, f\"shap_beeswarm_plot_{label}.png\"), dpi=300)\n            plt.show()\n\n    def _generate_diagnostic_report(self):\n        \"\"\"Generates a detailed text report suitable for diagnostics.\"\"\"\n        print(\"--> STAGE 7: Generating Diagnostic Report...\")\n        \n        report_path = os.path.join(self.output_dir, \"diagnostic_report.txt\")\n        \n        with open(report_path, \"w\") as f:\n            f.write(\"=\"*80 + \"\\n\")\n            f.write(\"AIMRAS: ADVANCED INDOOR MICROBIAL RISK ASSESSMENT - DIAGNOSTIC REPORT\\n\")\n            f.write(\"=\"*80 + \"\\n\\n\")\n\n            f.write(\"1. EXECUTIVE SUMMARY\\n\")\n            f.write(\"-\"*80 + \"\\n\")\n            f.write(\"This report details the performance and findings of the AIMRAS multi-label diagnostic model.\\n\")\n            f.write(f\"The system was trained to identify {len(self.target_labels)} specific, co-occurring environmental risks and comfort states.\\n\")\n            f.write(f\"Overall Jaccard Score (prediction set similarity): {self.report_data['overall_metrics']['jaccard_score']:.4f}\\n\")\n            f.write(f\"Overall Hamming Loss (label prediction error rate): {self.report_data['overall_metrics']['hamming_loss']:.4f}\\n\\n\")\n\n            f.write(\"2. DATA & RISK PROFILE\\n\")\n            f.write(\"-\"*80 + \"\\n\")\n            f.write(f\"Analysis conducted on {self.df.shape[0]} observations with {len(self.feature_names)} features.\\n\")\n            f.write(\"Initial prevalence of each state in the dataset (including untrainable states):\\n\")\n            f.write(self.report_data['initial_risk_distribution'].to_string())\n            f.write(\"\\n\\n\")\n\n            for label in self.target_labels:\n                f.write(f\"3. DETAILED ANALYSIS: {label.upper()}\\n\")\n                f.write(\"-\"*80 + \"\\n\")\n                f.write(\"Performance on the test set:\\n\")\n                f.write(self.report_data[f'report_{label}'])\n                f.write(\"\\n\")\n\n                # Find top SHAP features for this label\n                estimator = self.model.estimators_[self.target_labels.index(label)]\n                shap_values = shap.TreeExplainer(estimator).shap_values(pd.DataFrame(self.X_test, columns=self.feature_names))\n                \n                feature_importance = pd.DataFrame({\n                    'feature': self.feature_names,\n                    'mean_abs_shap': np.abs(shap_values).mean(axis=0)\n                }).sort_values('mean_abs_shap', ascending=False)\n                \n                f.write(\"Top 5 Predictive Factors (from SHAP analysis):\\n\")\n                f.write(feature_importance.head(5).to_string(index=False))\n                f.write(\"\\n\\n\")\n\n            f.write(\"4. CONCLUSION\\n\")\n            f.write(\"-\"*80 + \"\\n\")\n            f.write(\"The ClassifierChain model effectively identifies a comprehensive set of environmental states.\\n\")\n            f.write(\"Per-state SHAP analysis reveals distinct drivers for each condition, enabling targeted interventions.\\n\")\n            f.write(\"All supporting graphs are saved in the output directory.\\n\")\n\n        print(f\"âœ… Diagnostic report saved to: {report_path}\")\n\n    def predict_on_new_data(self, new_data: pd.DataFrame):\n        \"\"\"Predicts risk factors for new, unseen data using the saved model bundle.\"\"\"\n        print(\"--> PREDICTION: Running inference on new data...\")\n        model_path = os.path.join(self.output_dir, 'aimras_model_bundle.pkl')\n        \n        try:\n            bundle = joblib.load(model_path)\n            model, scaler, feature_names, target_labels = bundle['model'], bundle['scaler'], bundle['feature_names'], bundle['target_labels']\n        except FileNotFoundError:\n            print(f\"ERROR: Model bundle not found at {model_path}. Please run the training pipeline first.\")\n            return None\n        \n        # Pre-process new data\n        if 'timestamp' in new_data.columns:\n            new_data['timestamp'] = pd.to_datetime(new_data['timestamp'])\n            new_data = new_data.set_index('timestamp')\n        else:\n            # If no timestamp, create one for feature generation compatibility\n            new_data.index = pd.to_datetime(pd.date_range(start=pd.Timestamp.now(), periods=len(new_data), freq='H'))\n\n        # Clean column names to match training\n        new_data.rename(columns=lambda c: re.sub(r'\\(.*\\)', '', c).strip().lower().replace(' ', '_').replace('-', '_'), inplace=True)\n        \n        # Create a dummy history for rolling features if not available\n        # In a real application, you'd feed historical data\n        if len(new_data) == 1:\n            history_len = 24\n            temp_history = pd.concat([new_data] * history_len, ignore_index=True)\n            temp_history.index = pd.date_range(end=new_data.index[0], periods=history_len, freq='H')\n            new_data_featured = self._engineer_features_for_prediction(temp_history).iloc[-1:]\n        else:\n            new_data_featured = self._engineer_features_for_prediction(new_data)\n        \n        new_data_processed = new_data_featured.reindex(columns=feature_names, fill_value=0)\n        new_data_scaled = scaler.transform(new_data_processed)\n        \n        predictions_array = model.predict(new_data_scaled)\n        \n        results = []\n        for i, idx in enumerate(new_data.index):\n            result = {\"timestamp\": idx.isoformat()}\n            for j, label in enumerate(target_labels):\n                result[label] = \"Yes\" if predictions_array[i, j] == 1 else \"No\"\n            results.append(result)\n            print(f\"Prediction for {idx}: {result}\")\n            \n        return results\n\n    def _engineer_features_for_prediction(self, df: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Feature engineering pipeline specifically for prediction to avoid data leakage.\"\"\"\n        df_out = df.copy()\n        df_out['hour'] = df_out.index.hour\n        df_out['day_of_week'] = df_out.index.dayofweek\n        df_out['is_weekend'] = df_out['day_of_week'].isin([5, 6]).astype(int)\n        df_out['hour_sin'] = np.sin(2 * np.pi * df_out['hour'] / 24)\n        df_out['hour_cos'] = np.cos(2 * np.pi * df_out['hour'] / 24)\n        key_sensors = ['temperature', 'humidity', 'co2', 'pm2.5']\n        for col in key_sensors:\n            if col in df_out.columns:\n                df_out[f'{col}_24h_mean'] = df_out[col].rolling('24H', min_periods=1).mean()\n                df_out[f'{col}_1h_lag'] = df_out[col].shift(1)\n                df_out[f'{col}_rate_of_change'] = df_out[col].diff()\n        df_out['temp_humidity_interaction'] = df_out.get('temperature', 0) * df_out.get('humidity', 0)\n        df_out['dew_point'] = df_out.get('temperature', 0) - ((100 - df_out.get('humidity', 0)) / 5)\n        df_out['dew_point_spread'] = df_out.get('temperature', 0) - df_out['dew_point']\n        df_out = df_out.ffill().bfill().fillna(0)\n        return df_out\n\n    def run_full_pipeline(self):\n        \"\"\"Executes the entire analytics pipeline from start to finish.\"\"\"\n        print(\"ðŸš€ STARTING AIMRAS V11 FULL ANALYTICS PIPELINE ðŸš€\")\n        try:\n            stages = [\n                (\"Loading and Cleaning Data\", self._load_and_clean_data, self.config.get(\"data\", \"historical_file_path\")),\n                (\"Engineering Features\", self._engineer_features, None),\n                (\"Defining Risk Labels\", self._define_risk_and_comfort_labels, None),\n                (\"Preparing Data for Modeling\", self._prepare_for_modeling, None),\n                (\"Training and Saving Model\", self._train_and_save_model, None),\n                (\"Evaluating Model\", self._evaluate_model, None),\n                (\"Generating Diagnostic Report\", self._generate_diagnostic_report, None)\n            ]\n\n            for i, (name, func, arg) in enumerate(stages):\n                print(f\"\\n[STAGE {i+1}/{len(stages)}] {name}...\")\n                if arg: func(arg)\n                else: func()\n                print(f\"âœ… Stage {i+1} Complete\")\n\n            print(f\"\\n\\nðŸŽ‰ AIMRAS V11 PIPELINE COMPLETED SUCCESSFULLY! ðŸŽ‰\")\n            print(f\"All outputs saved in the '{self.output_dir}' directory.\")\n\n        except Exception:\n            print(f\"\\nâŒ PIPELINE FAILED! See the error traceback below.\")\n            traceback.print_exc()\n\n# ------------------------------------------------------------------------------\n# SECTION D: EXECUTION\n# ------------------------------------------------------------------------------\nif __name__ == '__main__':\n    engine = EnvironmentalDiagnosticEngine()\n    engine.run_full_pipeline()\n\n    # Demonstrate prediction on new data if the model was trained successfully\n    model_path = os.path.join(engine.output_dir, 'aimras_model_bundle.pkl')\n    if os.path.exists(model_path):\n        print(\"\\n\" + \"=\"*50)\n        print(\"DEMONSTRATING PREDICTION ON NEW (SIMULATED) DATA\")\n        print(\"=\"*50)\n        \n        # Simulate a scenario conducive to multiple risks\n        sample_data = {\n            'timestamp': [pd.Timestamp.now()],\n            'temperature': [25.5],\n            'humidity': [85.0], # High humidity\n            'co2': [1600],       # Poor ventilation\n            'pm2.5': [40.0],     # High PM2.5\n            'pm10': [55.0],\n            'tvoc': [300],\n            'co': [8.0],\n            'light_intensity': [200],\n            'motion_detected': [True],\n            'occupancy_count': [3]\n        }\n        sample_df = pd.DataFrame(sample_data)\n        \n        predictions = engine.predict_on_new_data(sample_df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Safe upgrade of required packages for AIMRAS\n!pip install --quiet --upgrade \\\n    numpy \\\n    pandas \\\n    scikit-learn \\\n    tensorflow \\\n    joblib \\\n    shap \\\n    seaborn \\\n    plotly \\\n    pywavelets \\\n    matplotlib \\\n    imbalanced-learn \\\n    xgboost || echo \"âš ï¸ Dependency conflicts detected with unused packages (e.g., ydata-profiling, bigframes). Safe to ignore.\"\n\n# Restart kernel to apply package changes\nimport os\nos.kill(os.getpid(), 9)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T04:59:53.480007Z","iopub.execute_input":"2025-07-07T04:59:53.480285Z","execution_failed":"2025-07-07T05:01:52.094Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m99.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m644.9/644.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.5/19.5 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m101.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.0.2 which is incompatible.\ngensim 4.3.3 requires scipy<1.14.0,>=1.7.0, but you have scipy 1.15.2 which is incompatible.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.0.2 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.0.2 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.0.2 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\nydata-profiling 4.16.1 requires matplotlib<=3.10,>=3.5, but you have matplotlib 3.10.3 which is incompatible.\ntensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.19.0 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.6.1 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ntf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.19.0 which is incompatible.\ntensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.19.0 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# PROJECT: ADVANCED INDOOR MICROBIAL RISK ASSESSMENT SYSTEM (AIMRAS)\n# FILE: aimras_lstm_trainer.py\n# VERSION: 12.8 (Regularized Deep Training Edition)\n#\n# PURPOSE:\n# This script trains a deeper, more heavily regularized Conv-LSTM network to\n# combat the overfitting observed in the previous version.\n#\n# KEY ARCHITECTURE CHANGES & FIXES:\n# 1. DEEPER ARCHITECTURE: The network has been made deeper with additional\n#    Conv1D and Bidirectional LSTM layers to learn more complex and abstract\n#    feature hierarchies, improving its ability to generalize.\n# 2. ADVANCED REGULARIZATION (CRITICAL FIX):\n#    - L2 Kernel Regularization: Added `l2(1e-4)` regularization to all key\n#      layers to penalize large weights and prevent the model from relying on\n#      any single feature.\n#    - Increased Dropout: Dropout rates have been increased throughout the\n#      network to further force the model to learn robust, redundant features.\n# 3. ANTI-OVERFITTING FOCUS: These changes directly target the overfitting\n#    problem, where the model was memorizing the synthetic training data but\n#    failing to perform well on unseen validation data.\n# ==============================================================================\n\n# ------------------------------------------------------------------------------\n# SECTION A: SETUP AND ENVIRONMENT\n# ------------------------------------------------------------------------------\n# Core package installation. imbalanced-learn is now required.\n# !pip install --quiet \\\n#      numpy==1.24.3 \\\n#      pandas==2.0.3 \\\n#      scikit-learn==1.3.2 \\\n#      tensorflow==2.12.0 \\\n#      imbalanced-learn==0.11.0 \\\n#      joblib==1.4.2 \\\n#      matplotlib==3.8.4\n\n# Core imports\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport os\nimport re\nimport warnings\nfrom typing import List, Tuple\nimport matplotlib.pyplot as plt\n\n# Scikit-learn, Imblearn, and TensorFlow/Keras\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import (\n    Input, LSTM, Dense, Dropout, Bidirectional, BatchNormalization, \n    Conv1D, MaxPooling1D\n)\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.metrics import AUC, Precision, Recall\n\n# Silence non-critical warnings and set seeds for reproducibility\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nSEED = 42\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\n\n# ------------------------------------------------------------------------------\n# SECTION B: DATA PROCESSING AND FEATURE ENGINEERING\n# ------------------------------------------------------------------------------\n\ndef load_and_clean_data(filepath: str) -> pd.DataFrame:\n    \"\"\"Loads, cleans, and standardizes the dataset.\"\"\"\n    print(\"--> Loading and Cleaning Data...\")\n    if not os.path.exists(filepath):\n        raise FileNotFoundError(f\"Dataset not found at {filepath}\")\n\n    df = pd.read_csv(filepath)\n    \n    def clean_col_name(col_name):\n        name = re.sub(r'\\(.*\\)', '', col_name).strip().lower().replace(' ', '_')\n        return name\n    \n    df.rename(columns=clean_col_name, inplace=True)\n    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n    df = df.dropna(subset=['timestamp']).set_index('timestamp').sort_index()\n    df = df[~df.index.duplicated(keep='last')]\n    \n    numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n    print(f\"Utilizing all {len(numeric_cols)} numeric columns: {numeric_cols}\")\n    \n    df[numeric_cols] = df[numeric_cols].interpolate(method='time').ffill().bfill()\n    return df\n\ndef define_risk_labels(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n    \"\"\"Defines a comprehensive suite of binary risk labels.\"\"\"\n    print(\"--> Defining Risk Labels...\")\n    df['Risk_High_PM2.5'] = (df['pm2.5'] > 35.4).astype(int)\n    df['Risk_High_CO'] = (df['co'] > 9.0).astype(int)\n    df['Risk_High_TVOC'] = (df['tvoc'] > 500).astype(int)\n    df['Risk_Poor_Ventilation'] = (df['co2'] > 1500).astype(int)\n    df['Comfort_Humidity_High'] = (df['humidity'] > 65.0).astype(int)\n    df['Comfort_Humidity_Low'] = (df['humidity'] < 30.0).astype(int)\n    df['Comfort_Thermal_Hot'] = (df['temperature'] > 27.0).astype(int)\n    df['Comfort_Thermal_Cold'] = (df['temperature'] < 20.0).astype(int)\n\n    all_labels = [\n        'Risk_High_PM2.5', 'Risk_High_CO', 'Risk_High_TVOC',\n        'Risk_Poor_Ventilation', 'Comfort_Humidity_High', 'Comfort_Humidity_Low',\n        'Comfort_Thermal_Hot', 'Comfort_Thermal_Cold'\n    ]\n    \n    trainable_labels = [label for label in all_labels if df[label].nunique() > 1]\n    print(f\"Found {len(trainable_labels)} trainable labels: {trainable_labels}\")\n    \n    return df, trainable_labels\n\ndef create_sequences(X: np.ndarray, y: np.ndarray, time_steps: int) -> Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Converts data into sequences for LSTM training.\"\"\"\n    print(f\"--> Creating sequences with {time_steps} time steps...\")\n    Xs, ys = [], []\n    for i in range(len(X) - time_steps):\n        Xs.append(X[i:(i + time_steps)])\n        ys.append(y[i + time_steps])\n    return np.array(Xs), np.array(ys)\n\n# ------------------------------------------------------------------------------\n# SECTION C: ADVANCED MODEL DEFINITION AND TRAINING\n# ------------------------------------------------------------------------------\n\ndef build_conv_lstm_model(input_shape: tuple, num_labels: int) -> tf.keras.Model:\n    \"\"\"Builds the deeper, regularized Conv-LSTM model.\"\"\"\n    print(\"--> Building Deeper, Regularized Conv-LSTM model...\")\n    \n    l2_reg = 1e-4 # Define the regularization factor\n    \n    inputs = Input(shape=input_shape)\n    \n    # --- Convolutional Feature Extraction Block ---\n    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(l2_reg))(inputs)\n    x = BatchNormalization()(x)\n    x = Conv1D(filters=64, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(l2_reg))(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(pool_size=2)(x)\n    x = Dropout(0.5)(x) # Increased dropout\n\n    x = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(l2_reg))(x)\n    x = BatchNormalization()(x)\n    x = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same', kernel_regularizer=l2(l2_reg))(x)\n    x = BatchNormalization()(x)\n    x = MaxPooling1D(pool_size=2)(x)\n    x = Dropout(0.5)(x) # Increased dropout\n\n    # --- LSTM Sequence Processing Block ---\n    x = Bidirectional(LSTM(units=128, return_sequences=True, kernel_regularizer=l2(l2_reg)))(x)\n    x = Dropout(0.5)(x)\n    \n    x = Bidirectional(LSTM(units=64, kernel_regularizer=l2(l2_reg)))(x)\n    x = Dropout(0.5)(x)\n    \n    # --- Final Prediction Block ---\n    x = Dense(units=64, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n    x = Dropout(0.5)(x)\n    outputs = Dense(units=num_labels, activation='sigmoid')(x)\n    \n    model = Model(inputs=inputs, outputs=outputs)\n    \n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005), # Slightly lower learning rate\n        loss='binary_crossentropy',\n        metrics=[\n            AUC(name='auc'),\n            Precision(name='precision'),\n            Recall(name='recall')\n        ]\n    )\n    model.summary()\n    return model\n\ndef plot_and_save_history(history: tf.keras.callbacks.History, output_dir: str):\n    \"\"\"Plots training history and saves the figures.\"\"\"\n    print(\"--> Generating and saving training history graphs...\")\n    metrics = ['loss', 'auc', 'precision', 'recall']\n    plt.style.use('seaborn-v0_8-whitegrid')\n\n    for metric in metrics:\n        plt.figure(figsize=(10, 6))\n        plt.plot(history.history[metric], label=f'Training {metric.capitalize()}')\n        plt.plot(history.history[f'val_{metric}'], label=f'Validation {metric.capitalize()}')\n        plt.title(f'Training and Validation {metric.capitalize()}', fontsize=16)\n        plt.xlabel('Epoch', fontsize=12)\n        plt.ylabel(metric.capitalize(), fontsize=12)\n        plt.legend()\n        \n        fig_path = os.path.join(output_dir, f'training_history_{metric}.png')\n        plt.savefig(fig_path, dpi=300)\n        print(f\"    - Saved {metric} plot to {fig_path}\")\n        plt.close()\n\ndef run_training_pipeline(data_path: str, output_dir: str, time_steps: int = 24):\n    \"\"\"Executes the full training pipeline.\"\"\"\n    print(f\"ðŸš€ STARTING AIMRAS V12.8 (REGULARIZED DEEP) TRAINING PIPELINE ðŸš€\")\n    os.makedirs(output_dir, exist_ok=True)\n\n    # 1. Load and process data\n    df = load_and_clean_data(data_path)\n    df, target_labels = define_risk_labels(df)\n    \n    feature_cols = df.select_dtypes(include=np.number).columns.drop(target_labels, errors='ignore').tolist()\n    \n    # 2. Scale features and prepare data\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(df[feature_cols])\n    y = df[target_labels].values\n    \n    # 3. Create sequences\n    X_seq, y_seq = create_sequences(X_scaled, y, time_steps)\n    if len(X_seq) == 0:\n        print(f\"âŒ ERROR: Not enough data to create sequences with {time_steps} steps.\")\n        return\n\n    # 4. Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X_seq, y_seq, test_size=0.2, random_state=SEED, shuffle=False\n    )\n    print(f\"Train/Test split complete. Original train shape: {X_train.shape}\")\n\n    # 5. Apply SMOTE using the robust combination-string method\n    print(\"--> Applying SMOTE to balance the training data...\")\n    n_samples, n_timesteps, n_features = X_train.shape\n    X_train_reshaped = X_train.reshape((n_samples, n_timesteps * n_features))\n    \n    y_combined = np.array([''.join(row.astype(str)) for row in y_train])\n    unique_classes, class_counts = np.unique(y_combined, return_counts=True)\n    \n    if len(unique_classes) > 1:\n        min_samples = min(class_counts)\n        k_neighbors = min(5, min_samples - 1) if min_samples > 1 else 1\n        if k_neighbors >= 1:\n            smote = SMOTE(random_state=SEED, k_neighbors=k_neighbors)\n            X_train_smote_flat, y_combined_smote = smote.fit_resample(X_train_reshaped, y_combined)\n            y_train_smote = np.array([list(label_str) for label_str in y_combined_smote], dtype=int)\n            X_train_smote = X_train_smote_flat.reshape((-1, n_timesteps, n_features))\n            print(f\"--> SMOTE complete. New balanced train shape: {X_train_smote.shape}\")\n        else:\n            print(\"--> SMOTE skipped. Not enough samples in the minority class.\")\n            X_train_smote, y_train_smote = X_train, y_train\n    else:\n        print(\"--> SMOTE skipped. Only one class combination present in training data.\")\n        X_train_smote, y_train_smote = X_train, y_train\n\n    # 6. Build and train the model\n    model = build_conv_lstm_model(input_shape=(X_train_smote.shape[1], X_train_smote.shape[2]), num_labels=len(target_labels))\n    \n    early_stopping = EarlyStopping(monitor='val_auc', mode='max', patience=10, restore_best_weights=True)\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n    \n    history = model.fit(\n        X_train_smote, y_train_smote,\n        epochs=50,\n        batch_size=128,\n        validation_data=(X_test, y_test),\n        callbacks=[early_stopping, reduce_lr],\n        verbose=1\n    )\n\n    # 7. Evaluate the final model\n    final_metrics = model.evaluate(X_test, y_test, return_dict=True)\n    print(\"\\n--- Final Model Evaluation on Test Set ---\")\n    for name, value in final_metrics.items():\n        print(f\"{name.capitalize():<12}: {value:.4f}\")\n\n    # 8. Generate and save training graphs\n    plot_and_save_history(history, output_dir)\n\n    # 9. Save the complete bundle\n    print(\"\\n--> Saving model and artifacts...\")\n    model.save(os.path.join(output_dir, 'aimras_lstm_model.keras'))\n    \n    bundle = {\n        'scaler': scaler,\n        'feature_names': feature_cols,\n        'target_labels': target_labels,\n        'time_steps': time_steps\n    }\n    joblib.dump(bundle, os.path.join(output_dir, 'aimras_lstm_bundle.pkl'))\n    \n    print(f\"ðŸŽ‰ TRAINING COMPLETE! Artifacts saved to '{output_dir}'.\")\n\n# ------------------------------------------------------------------------------\n# SECTION D: EXECUTION\n# ------------------------------------------------------------------------------\nif __name__ == '__main__':\n    DATA_FILE_PATH = '/kaggle/input/iot-indoor-air-quality/IoT_Indoor_Air_Quality_Dataset.csv'\n    OUTPUT_DIRECTORY = 'aimras_outputs_v13'\n    \n    TIME_STEPS = 24\n    \n    run_training_pipeline(DATA_FILE_PATH, OUTPUT_DIRECTORY, TIME_STEPS)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-07T05:02:02.396060Z","iopub.execute_input":"2025-07-07T05:02:02.396333Z"}},"outputs":[{"name":"stderr","text":"2025-07-07 05:02:04.009169: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751864524.031900     107 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751864524.038733     107 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1751864524.058102     107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1751864524.058121     107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1751864524.058124     107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1751864524.058126     107 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"ðŸš€ STARTING AIMRAS V12.8 (REGULARIZED DEEP) TRAINING PIPELINE ðŸš€\n--> Loading and Cleaning Data...\nUtilizing all 10 numeric columns: ['temperature', 'humidity', 'co2', 'pm2.5', 'pm10', 'tvoc', 'co', 'light_intensity', 'motion_detected', 'occupancy_count']\n--> Defining Risk Labels...\nFound 4 trainable labels: ['Risk_High_PM2.5', 'Comfort_Humidity_High', 'Comfort_Thermal_Hot', 'Comfort_Thermal_Cold']\n--> Creating sequences with 24 time steps...\nTrain/Test split complete. Original train shape: (77947, 24, 14)\n--> Applying SMOTE to balance the training data...\n--> SMOTE complete. New balanced train shape: (327408, 24, 14)\n--> Building Deeper, Regularized Conv-LSTM model...\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1751864540.741844     107 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"functional\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m14\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)              â”‚           \u001b[38;5;34m2,752\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)              â”‚             \u001b[38;5;34m256\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 â”‚                             â”‚                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)              â”‚          \u001b[38;5;34m12,352\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_1                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m24\u001b[0m, \u001b[38;5;34m64\u001b[0m)              â”‚             \u001b[38;5;34m256\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 â”‚                             â”‚                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)         â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m64\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)             â”‚          \u001b[38;5;34m24,704\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_2                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)             â”‚             \u001b[38;5;34m512\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 â”‚                             â”‚                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)                    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)             â”‚          \u001b[38;5;34m49,280\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_3                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)             â”‚             \u001b[38;5;34m512\u001b[0m â”‚\nâ”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 â”‚                             â”‚                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)              â”‚         \u001b[38;5;34m263,168\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m256\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚         \u001b[38;5;34m164,352\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (\u001b[38;5;33mDense\u001b[0m)                        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  â”‚           \u001b[38;5;34m8,256\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)                  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  â”‚               \u001b[38;5;34m0\u001b[0m â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   â”‚             \u001b[38;5;34m260\u001b[0m â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\nâ”ƒ<span style=\"font-weight: bold\"> Layer (type)                         </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape                </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ\nâ”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\nâ”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,752</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 â”‚                             â”‚                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_1                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">24</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 â”‚                             â”‚                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)         â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_2                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 â”‚                             â”‚                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             â”‚          <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ batch_normalization_3                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> â”‚\nâ”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 â”‚                             â”‚                 â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">263,168</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">164,352</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">260</span> â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m526,660\u001b[0m (2.01 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">526,660</span> (2.01 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m525,892\u001b[0m (2.01 MB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">525,892</span> (2.01 MB)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m768\u001b[0m (3.00 KB)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">768</span> (3.00 KB)\n</pre>\n"},"metadata":{}},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1751864553.339058     145 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 17ms/step - auc: 0.6353 - loss: 0.7262 - precision: 0.5597 - recall: 0.3590 - val_auc: 0.7113 - val_loss: 0.5387 - val_precision: 0.5139 - val_recall: 0.5537 - learning_rate: 5.0000e-04\nEpoch 2/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 16ms/step - auc: 0.6797 - loss: 0.6447 - precision: 0.5818 - recall: 0.4517 - val_auc: 0.7297 - val_loss: 0.5097 - val_precision: 0.5484 - val_recall: 0.5666 - learning_rate: 5.0000e-04\nEpoch 3/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 17ms/step - auc: 0.7118 - loss: 0.6198 - precision: 0.5951 - recall: 0.5374 - val_auc: 0.7267 - val_loss: 0.5156 - val_precision: 0.5419 - val_recall: 0.5663 - learning_rate: 5.0000e-04\nEpoch 4/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 17ms/step - auc: 0.7292 - loss: 0.6066 - precision: 0.6059 - recall: 0.5658 - val_auc: 0.7325 - val_loss: 0.5050 - val_precision: 0.5541 - val_recall: 0.5715 - learning_rate: 5.0000e-04\nEpoch 5/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 17ms/step - auc: 0.7391 - loss: 0.5995 - precision: 0.6132 - recall: 0.5796 - val_auc: 0.7262 - val_loss: 0.5156 - val_precision: 0.5442 - val_recall: 0.5705 - learning_rate: 5.0000e-04\nEpoch 6/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 17ms/step - auc: 0.7471 - loss: 0.5932 - precision: 0.6199 - recall: 0.5855 - val_auc: 0.7319 - val_loss: 0.5148 - val_precision: 0.5468 - val_recall: 0.5702 - learning_rate: 5.0000e-04\nEpoch 7/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 16ms/step - auc: 0.7528 - loss: 0.5893 - precision: 0.6259 - recall: 0.5924 - val_auc: 0.7231 - val_loss: 0.5253 - val_precision: 0.5391 - val_recall: 0.5724 - learning_rate: 5.0000e-04\nEpoch 8/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 17ms/step - auc: 0.7581 - loss: 0.5851 - precision: 0.6297 - recall: 0.5988 - val_auc: 0.7350 - val_loss: 0.5133 - val_precision: 0.5499 - val_recall: 0.5712 - learning_rate: 5.0000e-04\nEpoch 9/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 17ms/step - auc: 0.7611 - loss: 0.5829 - precision: 0.6317 - recall: 0.6039 - val_auc: 0.7296 - val_loss: 0.5123 - val_precision: 0.5485 - val_recall: 0.5726 - learning_rate: 5.0000e-04\nEpoch 10/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 17ms/step - auc: 0.7717 - loss: 0.5719 - precision: 0.6403 - recall: 0.6149 - val_auc: 0.7240 - val_loss: 0.5190 - val_precision: 0.5416 - val_recall: 0.5731 - learning_rate: 1.0000e-04\nEpoch 11/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 17ms/step - auc: 0.7789 - loss: 0.5631 - precision: 0.6466 - recall: 0.6239 - val_auc: 0.7269 - val_loss: 0.5158 - val_precision: 0.5428 - val_recall: 0.5720 - learning_rate: 1.0000e-04\nEpoch 12/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 17ms/step - auc: 0.7827 - loss: 0.5586 - precision: 0.6511 - recall: 0.6265 - val_auc: 0.7261 - val_loss: 0.5168 - val_precision: 0.5413 - val_recall: 0.5724 - learning_rate: 1.0000e-04\nEpoch 13/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 17ms/step - auc: 0.7848 - loss: 0.5560 - precision: 0.6522 - recall: 0.6309 - val_auc: 0.7248 - val_loss: 0.5164 - val_precision: 0.5396 - val_recall: 0.5733 - learning_rate: 1.0000e-04\nEpoch 14/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 17ms/step - auc: 0.7874 - loss: 0.5528 - precision: 0.6554 - recall: 0.6307 - val_auc: 0.7250 - val_loss: 0.5176 - val_precision: 0.5379 - val_recall: 0.5734 - learning_rate: 1.0000e-04\nEpoch 15/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 17ms/step - auc: 0.7896 - loss: 0.5505 - precision: 0.6588 - recall: 0.6326 - val_auc: 0.7260 - val_loss: 0.5168 - val_precision: 0.5387 - val_recall: 0.5744 - learning_rate: 2.0000e-05\nEpoch 16/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 17ms/step - auc: 0.7912 - loss: 0.5485 - precision: 0.6591 - recall: 0.6351 - val_auc: 0.7251 - val_loss: 0.5172 - val_precision: 0.5377 - val_recall: 0.5744 - learning_rate: 2.0000e-05\nEpoch 17/50\n\u001b[1m2558/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 17ms/step - auc: 0.7922 - loss: 0.5474 - precision: 0.6598 - recall: 0.6356 - val_auc: 0.7253 - val_loss: 0.5166 - val_precision: 0.5379 - val_recall: 0.5739 - learning_rate: 2.0000e-05\nEpoch 18/50\n\u001b[1m2139/2558\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m6s\u001b[0m 16ms/step - auc: 0.7929 - loss: 0.5463 - precision: 0.6614 - recall: 0.6354","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}